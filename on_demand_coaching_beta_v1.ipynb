{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"[Time Tracker] `{func.__name__}` took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "\n",
    "# print(PINECONE_API)\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(\"potential-talents\")  # -- COMPLETE SURGICAL TECH BOOTCAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `get_embedding` took 0.1646 seconds\n",
      "384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\Apziva Projects\\Project 3\\Potential-Talents\\env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "@track_time\n",
    "def get_embedding(text=\"None\"):\n",
    "    # Generate embedding using the pre-loaded model\n",
    "    embedding = sbert_model.encode(text)\n",
    "\n",
    "    # Return the embedding as a list/array\n",
    "    return embedding.tolist()\n",
    "\n",
    "#print(len(get_embedding(\"Student\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `get_embedding` took 0.0074 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.1179 seconds\n",
      "[{'id': 'vec_15',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'row_id': 15.0,\n",
      "              'source': './potential-talents.xlsx',\n",
      "              'text': 'id: 16 job_title: Native English Teacher at EPIK '\n",
      "                      '(English Program in Korea) location: Kanada connection: '\n",
      "                      '500+  fit: nan'},\n",
      " 'score': 0.334692508,\n",
      " 'values': []}, {'id': 'vec_31',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'row_id': 31.0,\n",
      "              'source': './potential-talents.xlsx',\n",
      "              'text': 'id: 32 job_title: Native English Teacher at EPIK '\n",
      "                      '(English Program in Korea) location: Kanada connection: '\n",
      "                      '500+  fit: nan'},\n",
      " 'score': 0.334044933,\n",
      " 'values': []}, {'id': 'vec_95',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'row_id': 95.0,\n",
      "              'source': './potential-talents.xlsx',\n",
      "              'text': 'id: 96 job_title: Student at Indiana University Kokomo '\n",
      "                      '- Business Management - \\n'\n",
      "                      'Retail Manager at Delphi Hardware and Paint location: '\n",
      "                      'Lafayette, Indiana connection: 19 fit: nan'},\n",
      " 'score': 0.32843098,\n",
      " 'values': []}, {'id': 'vec_1',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'row_id': 1.0,\n",
      "              'source': './potential-talents.xlsx',\n",
      "              'text': 'id: 2 job_title: Native English Teacher at EPIK '\n",
      "                      '(English Program in Korea) location: Kanada connection: '\n",
      "                      '500+  fit: nan'},\n",
      " 'score': 0.324119657,\n",
      " 'values': []}, {'id': 'vec_44',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'row_id': 44.0,\n",
      "              'source': './potential-talents.xlsx',\n",
      "              'text': 'id: 45 job_title: Native English Teacher at EPIK '\n",
      "                      '(English Program in Korea) location: Kanada connection: '\n",
      "                      '500+  fit: nan'},\n",
      " 'score': 0.32262218,\n",
      " 'values': []}]\n"
     ]
    }
   ],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "@track_time\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "    return result[\"matches\"]\n",
    "\n",
    "print(query_pinecone(get_embedding(\"Student\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to count number of tokens\n",
    "\"\"\"\n",
    "Putting tokenizer outside of the function to avoid reinitialization and optimize performance.\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "@track_time\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimilarityModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (key): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the finetuned model from NLP_OPS\n",
    "model_path = \"finetuned_job_title_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Load the similarity head\n",
    "similarity_head_path = os.path.join(model_path, \"similarity_head.pt\")\n",
    "if os.path.exists(similarity_head_path):\n",
    "    similarity_head = torch.nn.Linear(base_model.config.hidden_size, 1)\n",
    "    similarity_head.load_state_dict(\n",
    "        torch.load(similarity_head_path, map_location=torch.device(\"cpu\"))\n",
    "    )\n",
    "else:\n",
    "    # Create a new head if file doesn't exist\n",
    "    similarity_head = torch.nn.Linear(base_model.config.hidden_size, 1)\n",
    "\n",
    "\n",
    "# Create the full model\n",
    "class SimilarityModel(torch.nn.Module):\n",
    "    def __init__(self, encoder, head):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.encoder(**inputs)\n",
    "        if hasattr(outputs, \"last_hidden_state\"):\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            embeddings = outputs[0].mean(dim=1)\n",
    "        return self.head(embeddings)\n",
    "\n",
    "\n",
    "model = SimilarityModel(base_model, similarity_head)\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_21804\\693709855.py:133: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n",
      "d:\\Repositories\\Apziva Projects\\Project 3\\Potential-Talents\\env\\Lib\\site-packages\\gradio\\layouts\\column.py:59: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `create_gradio_interface` took 0.1318 seconds\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://823f81a01c70a47e14.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://823f81a01c70a47e14.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `get_embedding` took 0.0060 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.1748 seconds\n",
      "[Time Tracker] `get_model_response` took 0.3466 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `get_embedding` took 0.0167 seconds\n",
      "[Time Tracker] `query_pinecone` took 0.2109 seconds\n",
      "[Time Tracker] `get_model_response` took 0.6140 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `get_embedding` took 0.0096 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.1781 seconds\n",
      "[Time Tracker] `get_model_response` took 1.8151 seconds\n"
     ]
    }
   ],
   "source": [
    "@track_time\n",
    "def get_model_response(query, candidates, system_prompt=\"\", user_prompt=\"\"):\n",
    "    \"\"\"Get response using the finetuned model\"\"\"\n",
    "    # If prompts are provided, we can use them to enhance the query\n",
    "    enhanced_query = query\n",
    "    if system_prompt and user_prompt:\n",
    "        # You could combine the prompts with the query if your model benefits from it\n",
    "        enhanced_query = f\"{system_prompt}\\n\\n{user_prompt}\\n\\n{query}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode the query\n",
    "        query_inputs = tokenizer(\n",
    "            enhanced_query, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "\n",
    "        # Get similarity scores for each candidate\n",
    "        scores = []\n",
    "        for candidate in candidates:\n",
    "            # Encode the candidate\n",
    "            candidate_inputs = tokenizer(\n",
    "                candidate, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "\n",
    "            # Get embeddings\n",
    "            query_embedding = model.encoder(**query_inputs)[0].mean(dim=1)\n",
    "            candidate_embedding = model.encoder(**candidate_inputs)[0].mean(dim=1)\n",
    "\n",
    "            # Calculate similarity score\n",
    "            similarity = torch.nn.functional.cosine_similarity(\n",
    "                query_embedding, candidate_embedding\n",
    "            )\n",
    "            scores.append(similarity.item())\n",
    "\n",
    "        # Sort candidates by score\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        sorted_candidates = [candidates[i] for i in sorted_indices]\n",
    "        sorted_scores = [scores[i] for i in sorted_indices]\n",
    "\n",
    "        # Format the response\n",
    "        response = \"Here are the most relevant matches:\\n\\n\"\n",
    "        for i, (candidate, score) in enumerate(\n",
    "            zip(sorted_candidates[:5], sorted_scores[:5])\n",
    "        ):\n",
    "            response += f\"{i + 1}. {candidate} (Score: {score:.3f})\\n\"\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "@track_time\n",
    "def process_user_query(user_query: str, conversation_history: list, response_type: str):\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "\n",
    "    # Extract text and metadata from chunks\n",
    "    candidates = [chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks]\n",
    "    context = \"\\n\".join(candidates)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nAssistant: {response}\"\n",
    "        for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an AI Talent Hunter that helps find the best candidates based on job requirements.\n",
    "    \n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "    \n",
    "    Candidate database:\n",
    "    {context}\n",
    "    \n",
    "    Analyze the user's query and find the most relevant candidates from the database.\n",
    "    \"\"\"\n",
    "\n",
    "    # User prompt\n",
    "    user_prompt = f\"\"\"\n",
    "    New recruitment query:\n",
    "    \"{user_query}\"\n",
    "    \n",
    "    Response type requested: {response_type}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get response from finetuned model\n",
    "    model_response = get_model_response(\n",
    "        user_query, candidates, system_prompt, user_prompt\n",
    "    )\n",
    "\n",
    "    # Combine with context for display\n",
    "    full_response = f\"\"\"Based on your query about \"{user_query}\", I've found these potential matches:\n",
    "\n",
    "{model_response}\n",
    "\n",
    "This analysis is based on our finetuned talent matching model that evaluates semantic similarity between your query and candidate profiles.\n",
    "\"\"\"\n",
    "\n",
    "    # First, yield a response with empty text to set up the message\n",
    "    temp_history = conversation_history.copy()\n",
    "    temp_history.append((user_query, \"\"))\n",
    "    yield temp_history, context\n",
    "\n",
    "    # Simulate streaming by yielding chunks of the response\n",
    "    chunks = [full_response[i : i + 20] for i in range(0, len(full_response), 20)]\n",
    "    partial_response = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        partial_response += chunk\n",
    "        temp_history = conversation_history.copy()\n",
    "        temp_history.append((user_query, partial_response))\n",
    "        yield temp_history, context\n",
    "        time.sleep(0.01)  # Small delay to simulate streaming\n",
    "\n",
    "    # Return the final history with the complete response\n",
    "    final_history = conversation_history.copy()\n",
    "    final_history.append((user_query, full_response))\n",
    "    yield final_history, context\n",
    "\n",
    "\n",
    "@track_time\n",
    "def create_gradio_interface(conversation_history, response_type=\"default\"):\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"# 🔍 AI Talent Hunter\")\n",
    "        gr.Markdown(\n",
    "            \"Welcome! I'll help you find the perfect candidates. Describe the position or skills you're looking for.\"\n",
    "        )\n",
    "\n",
    "        # State management\n",
    "        chat_history = gr.State(conversation_history)\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            with gr.Column(scale=0.5):\n",
    "                context_display = gr.Textbox(\n",
    "                    label=\"Candidate Database Results\", interactive=False\n",
    "                )\n",
    "\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Your Recruitment Query\",\n",
    "            placeholder=\"E.g., 'Find me experienced surgical technicians' or 'I need candidates with healthcare administration skills'\",\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Search Candidates\", variant=\"primary\")\n",
    "            undo_btn = gr.Button(\"Undo Last\")\n",
    "            clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        def handle_submit(user_query, history):\n",
    "            if not user_query.strip():\n",
    "                return gr.update(), history, \"\"\n",
    "\n",
    "            # Use the generator directly from process_user_query\n",
    "            # This will yield incremental updates as they arrive\n",
    "            response_generator = process_user_query(user_query, history, response_type)\n",
    "\n",
    "            for updated_history, context in response_generator:\n",
    "                # Directly update the chatbot with each streaming chunk\n",
    "                yield \"\", updated_history, context, updated_history\n",
    "\n",
    "        # Component interactions with streaming support\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        # Add submit on Enter key press\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda history: history[:-1] if history else [],\n",
    "            [chat_history],\n",
    "            [chat_history],\n",
    "        ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [], None, [chat_history]).then(\n",
    "            lambda: ([], \"\"), None, [chatbot, context_display]\n",
    "        )\n",
    "\n",
    "    return interface\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the application.\n",
    "\n",
    "    Initializes the conversation history with a welcome message,\n",
    "    creates the Gradio interface, and launches the web app.\n",
    "    \"\"\"\n",
    "    # Initialize conversation history with welcome message\n",
    "    welcome_message = \"Hello! I'm your AI Talent Hunter. I can help you find the perfect candidates for your positions by analyzing our talent database. What kind of talent are you looking for today?\"\n",
    "    initial_conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    # Create and launch the interface\n",
    "    interface = create_gradio_interface(initial_conversation_history, \"long\")\n",
    "    interface.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
