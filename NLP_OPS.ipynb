{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8268e07",
   "metadata": {},
   "source": [
    "# NLP Operations: Job Title Matching\n",
    "This notebook demonstrates various NLP techniques to vectorize job titles and a search term, and then ranks candidates by similarity. Techniques covered:\n",
    "- TF-IDF\n",
    "- Word2Vec (Google)\n",
    "- GloVe\n",
    "- FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "840e55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from dotenv import load_dotenv\n",
    "from utils import bleu_score\n",
    "from collections import Counter\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    ")  # Import METEOR function from utils\n",
    "from utils import meteor\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Import custom utility functions\n",
    "\n",
    "# For CIDEr, we'll implement a simplified version\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735f83d",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Load job titles from the Excel file and define a search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0b36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected search term: Student\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"potential-talents.xlsx\")\n",
    "possible_columns = [\n",
    "    \"job_title\",\n",
    "    \"title\",\n",
    "    \"position\",\n",
    "    \"role\",\n",
    "    \"job\",\n",
    "    \"designation\",\n",
    "    \"job title\",\n",
    "]\n",
    "job_title_column = None\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_columns):\n",
    "        job_title_column = col\n",
    "        break\n",
    "if not job_title_column:\n",
    "    raise ValueError(\"Job title column not found. Please specify it manually.\")\n",
    "job_titles = df[job_title_column].dropna().astype(str).tolist()\n",
    "\n",
    "# Filter job titles to only those with 1 or 2 words\n",
    "filtered_job_titles = [title for title in job_titles if 1 <= len(title.split()) <= 2]\n",
    "\n",
    "# Randomly select a search term from filtered job titles\n",
    "if filtered_job_titles:\n",
    "    # search_term = random.choice(filtered_job_titles)\n",
    "    search_term = \"Student\"  # saving for maintaining consistency\n",
    "else:\n",
    "    raise ValueError(\"No job titles with 1 or 2 words found.\")\n",
    "\n",
    "print(f\"Randomly selected search term: {search_term}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2b628",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Vectorization & Cosine Similarity\n",
    "Vectorize job titles and search term using TF-IDF, then rank candidates by similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78bd229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by TF-IDF similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = job_titles + [search_term]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "search_vec = X[-1]\n",
    "job_vecs = X[:-1]\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by TF-IDF similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af1a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- LLM-based Candidate Ranking using Groq API (Llama 3 70B Versatile) ---\n",
    "# import requests\n",
    "# import os\n",
    "# import json\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# def rank_candidates_with_llm(\n",
    "#     job_titles, search_term, model=\"llama3-70b-8192\"\n",
    "# ):  # Llama 3 70B Versatile\n",
    "#     \"\"\"\n",
    "#     Use Groq LLM API to rank job titles by relevance to the search term.\n",
    "#     Args:\n",
    "#         job_titles (list): List of job title strings.\n",
    "#         search_term (str): The search term/job title to match against.\n",
    "#         model (str): Groq model name (default: llama3-70b-8192).\n",
    "#     Returns:\n",
    "#         list: Ranked job titles (most relevant first).\n",
    "#     \"\"\"\n",
    "#     api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "#     if not api_key:\n",
    "#         raise ValueError(\n",
    "#             \"GROQ_API_KEY not found in environment variables. Please set it in your .env file.\"\n",
    "#         )\n",
    "#     prompt = f\"\"\"\n",
    "# You are an expert recruiter. Given the following list of candidate job titles, rank them from most to least relevant for the search term: '{search_term}'.\\n\\nJob Titles:\\n\"\"\"\n",
    "#     for i, title in enumerate(job_titles, 1):\n",
    "#         prompt += f\"{i}. {title}\\n\"\n",
    "#     prompt += \"\\nReturn the ranking as a numbered list, most relevant first. Only include the job titles.\"\n",
    "\n",
    "#     headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "#     data = {\n",
    "#         \"model\": model,\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#         \"temperature\": 0.2,\n",
    "#     }\n",
    "#     response = requests.post(\n",
    "#         \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "#         headers=headers,\n",
    "#         data=json.dumps(data),\n",
    "#     )\n",
    "#     response.raise_for_status()\n",
    "#     result = response.json()\n",
    "#     llm_output = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "#     # Parse the LLM output into a ranked list\n",
    "#     ranked = [\n",
    "#         line.split(\". \", 1)[-1].strip()\n",
    "#         for line in llm_output.split(\"\\n\")\n",
    "#         if line.strip() and line[0].isdigit()\n",
    "#     ]\n",
    "#     return ranked\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # ranked_list = rank_candidates_with_llm(job_titles, search_term)\n",
    "# # print(\"LLM-ranked job titles:\")\n",
    "# # for title in ranked_list:\n",
    "# #     print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0288f",
   "metadata": {},
   "source": [
    "## 4. Word2Vec (Google News) Vectorization & Cosine Similarity\n",
    "Vectorize using pre-trained Google News Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6bc6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected search term: Student\n",
      "Top 10 job titles by Word2Vec similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Westfield State University (Score: 0.793)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n"
     ]
    }
   ],
   "source": [
    "# Download Google News vectors (only needs to be done once)\n",
    "# w2v = api.load('word2vec-google-news-300')\n",
    "w2v = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "\n",
    "def get_w2v_vector(text, model):\n",
    "    words = [w for w in nltk.word_tokenize(text.lower()) if w in model]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[w] for w in words], axis=0)\n",
    "\n",
    "\n",
    "# Load the Excel file containing potential talents data\n",
    "# (Assumes the file is in the same directory as the notebook)\n",
    "df = pd.read_excel(\"potential-talents.xlsx\")\n",
    "\n",
    "# List of possible column names that may contain job titles\n",
    "possible_columns = [\n",
    "    \"job_title\",\n",
    "    \"title\",\n",
    "    \"position\",\n",
    "    \"role\",\n",
    "    \"job\",\n",
    "    \"designation\",\n",
    "    \"job title\",\n",
    "]\n",
    "\n",
    "# Initialize variable to store the detected job title column name\n",
    "job_title_column = None\n",
    "# Loop through columns in the DataFrame to find a matching job title column\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_columns):\n",
    "        job_title_column = col  # Set the column name if a match is found\n",
    "        break\n",
    "# Raise an error if no job title column is found\n",
    "default_job_title_error = \"Job title column not found. Please specify it manually.\"\n",
    "if not job_title_column:\n",
    "    raise ValueError(default_job_title_error)\n",
    "\n",
    "# Extract job titles as a list of strings, dropping missing values\n",
    "job_titles = df[job_title_column].dropna().astype(str).tolist()\n",
    "\n",
    "# Filter job titles to only those with 1 or 2 words\n",
    "filtered_job_titles = [title for title in job_titles if 1 <= len(title.split()) <= 2]\n",
    "\n",
    "# Randomly select a search term from filtered job titles\n",
    "if filtered_job_titles:\n",
    "    search_term = random.choice(filtered_job_titles)\n",
    "else:\n",
    "    raise ValueError(\"No job titles with 1 or 2 words found.\")\n",
    "\n",
    "# Print the randomly selected search term\n",
    "print(f\"Randomly selected search term: {search_term}\")\n",
    "\n",
    "job_vecs = np.array([get_w2v_vector(title, w2v) for title in job_titles])\n",
    "search_vec = get_w2v_vector(search_term, w2v).reshape(1, -1)\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by Word2Vec similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e7eef",
   "metadata": {},
   "source": [
    "## 5. GloVe Vectorization & Cosine Similarity\n",
    "Vectorize using pre-trained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d2bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by GloVe similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Westfield State University (Score: 0.699)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n"
     ]
    }
   ],
   "source": [
    "# Download GloVe vectors (only needs to be done once)\n",
    "# glove = api.load('glove-wiki-gigaword-300')\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "\n",
    "def get_glove_vector(text, model):\n",
    "    words = [w for w in nltk.word_tokenize(text.lower()) if w in model]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[w] for w in words], axis=0)\n",
    "\n",
    "\n",
    "job_vecs = np.array([get_glove_vector(title, glove) for title in job_titles])\n",
    "search_vec = get_glove_vector(search_term, glove).reshape(1, -1)\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by GloVe similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c981e80",
   "metadata": {},
   "source": [
    "## 6. FastText Vectorization & Cosine Similarity\n",
    "Vectorize using pre-trained FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd5538c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by FastText similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Chapman University (Score: 0.709)\n",
      "Student at Chapman University (Score: 0.709)\n"
     ]
    }
   ],
   "source": [
    "# Download FastText vectors (only needs to be done once)\n",
    "# fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "\n",
    "def get_fasttext_vector(text, model):\n",
    "    words = [w for w in nltk.word_tokenize(text.lower()) if w in model]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[w] for w in words], axis=0)\n",
    "\n",
    "\n",
    "job_vecs = np.array(\n",
    "    [get_fasttext_vector(title, fasttext_model) for title in job_titles]\n",
    ")\n",
    "search_vec = get_fasttext_vector(search_term, fasttext_model).reshape(1, -1)\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by FastText similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82dca8",
   "metadata": {},
   "source": [
    "## 11. Transformer-based Contextual Embeddings (BERT/Sentence-BERT)\n",
    "Use Sentence-BERT to generate contextual embeddings for job titles and the search term, then rank by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd69527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by SBERT similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Westfield State University (Score: 0.616)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Indiana University Kokomo - Business Management - \n",
      "Retail Manager at Delphi Hardware and Paint (Score: 0.409)\n",
      "Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "Advisory Board Member at Celal Bayar University (Score: 0.398)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained Sentence-BERT model\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute embeddings\n",
    "job_embeddings = sbert_model.encode(job_titles)\n",
    "search_embedding = sbert_model.encode([search_term])\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(search_embedding, job_embeddings).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "print(\"Top 10 job titles by SBERT similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51b2e2",
   "metadata": {},
   "source": [
    "## 7. BLEU Score Calculation\n",
    "Calculate BLEU score for semantic similarity between search term and job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d238ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by BLEU semantic similarity to search term:\n",
      "Student (BLEU Score: 1.000)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Westfield State University (BLEU Score: 0.046)\n",
      "Aspiring Human Resources Management student seeking an internship (BLEU Score: 0.029)\n",
      "Aspiring Human Resources Management student seeking an internship (BLEU Score: 0.029)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (BLEU Score: 0.026)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (BLEU Score: 0.026)\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score for each job title against the search term\n",
    "smoothie = SmoothingFunction().method4\n",
    "search_tokens = nltk.word_tokenize(search_term.lower())\n",
    "bleu_scores = [\n",
    "    sentence_bleu(\n",
    "        [search_tokens], nltk.word_tokenize(title.lower()), smoothing_function=smoothie\n",
    "    )\n",
    "    for title in job_titles\n",
    "]\n",
    "ranked_indices = np.argsort(bleu_scores)[::-1]\n",
    "print(\"Top 10 job titles by BLEU semantic similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (BLEU Score: {bleu_scores[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f4e3d",
   "metadata": {},
   "source": [
    "## 8. METEOR Score Calculation\n",
    "Calculate METEOR score for semantic similarity. METEOR considers synonyms and stemming, making it more suitable for semantic similarity than BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b8c5f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by METEOR semantic similarity to search term:\n",
      "Student (METEOR Score: 0.500)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Westfield State University (METEOR Score: 0.357)\n",
      "Aspiring Human Resources Management student seeking an internship (METEOR Score: 0.294)\n",
      "Aspiring Human Resources Management student seeking an internship (METEOR Score: 0.294)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (METEOR Score: 0.278)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (METEOR Score: 0.278)\n"
     ]
    }
   ],
   "source": [
    "# Calculate METEOR score for each job title against the search term\n",
    "search_tokens = nltk.word_tokenize(search_term.lower())\n",
    "meteor_scores = [\n",
    "    meteor_score([search_tokens], nltk.word_tokenize(title.lower()))\n",
    "    for title in job_titles\n",
    "]\n",
    "meteor_rank = np.argsort(meteor_scores)[::-1]\n",
    "\n",
    "print(\"Top 10 job titles by METEOR semantic similarity to search term:\")\n",
    "for idx in meteor_rank[:10]:\n",
    "    print(f\"{job_titles[idx]} (METEOR Score: {meteor_scores[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f5a8b",
   "metadata": {},
   "source": [
    "## 9. CIDEr Score Calculation\n",
    "Calculate CIDEr (Consensus-based Image Description Evaluation) score. Originally for image captioning, but useful for semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d3e1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by CIDEr semantic similarity to search term:\n",
      "Student (CIDEr Score: 1.000)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Chapman University (CIDEr Score: 0.105)\n",
      "Student at Chapman University (CIDEr Score: 0.105)\n"
     ]
    }
   ],
   "source": [
    "# Import optimized CiderScorer from utils\n",
    "from utils import CiderScorer\n",
    "\n",
    "# Instantiate one CiderScorer with job_titles once to avoid O(NÂ²) IDF recomputation\n",
    "cider = CiderScorer(job_titles)\n",
    "cider_scores = [cider.score(search_term, t) for t in job_titles]\n",
    "cider_rank = np.argsort(cider_scores)[::-1]\n",
    "\n",
    "print(\"Top 10 job titles by CIDEr semantic similarity to search term:\")\n",
    "for idx in cider_rank[:10]:\n",
    "    print(f\"{job_titles[idx]} (CIDEr Score: {cider_scores[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e9c7a",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Metric Comparison\n",
    "Compare all methods and recommend the best approach for job title semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7f2b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE COMPARISON OF SEMANTIC SIMILARITY METRICS ===\n",
      "\n",
      "Top match from each method:\n",
      "\n",
      "TF-IDF + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "Word2Vec + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "GloVe + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "FastText + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "BLEU Score:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "METEOR Score:\n",
      "  Job Title: Student\n",
      "  Score: 0.500\n",
      "\n",
      "CIDEr Score:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "=== RECOMMENDATION ===\n",
      "\n",
      "For job title semantic similarity, here's the ranking of methods:\n",
      "\n",
      "1. **GloVe + Cosine Similarity** (BEST CHOICE)\n",
      "   - Excellent semantic understanding\n",
      "   - Good balance of performance and accuracy\n",
      "   - Handles out-of-vocabulary words reasonably\n",
      "2. **Word2Vec + Cosine Similarity** (Second Choice)\n",
      "   - Strong semantic relationships\n",
      "   - Trained on Google News, good for professional terms\n",
      "3. **FastText + Cosine Similarity** (Third Choice)\n",
      "   - Handles subword information well\n",
      "   - Good for rare or misspelled words\n",
      "4. **METEOR Score** (Best for text generation evaluation)\n",
      "   - Considers synonyms and stemming\n",
      "   - Better than BLEU for semantic similarity\n",
      "5. **CIDEr Score** (Good for consensus-based evaluation)\n",
      "   - Uses TF-IDF weighting\n",
      "   - Good when you have multiple reference texts\n",
      "6. **TF-IDF + Cosine Similarity** (Baseline)\n",
      "   - Simple and fast\n",
      "   - Limited semantic understanding\n",
      "7. **BLEU Score** (Not recommended for this task)\n",
      "   - Designed for machine translation\n",
      "   - Poor for semantic similarity of short texts\n",
      "\n",
      "**FINAL RECOMMENDATION: Use GloVe + Cosine Similarity**\n",
      "This method provides the best balance of semantic understanding,\n",
      "computational efficiency, and practical performance for job title matching.\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive comparison\n",
    "print(\"=== COMPREHENSIVE COMPARISON OF SEMANTIC SIMILARITY METRICS ===\\n\")\n",
    "\n",
    "# Get top result from each method\n",
    "methods = {\n",
    "    \"TF-IDF + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"Word2Vec + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"GloVe + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"FastText + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"BLEU Score\": (np.argsort(bleu_scores)[::-1], bleu_scores),\n",
    "    \"METEOR Score\": (np.argsort(meteor_scores)[::-1], meteor_scores),\n",
    "    \"CIDEr Score\": (np.argsort(cider_scores)[::-1], cider_scores),\n",
    "}\n",
    "\n",
    "print(\"Top match from each method:\")\n",
    "for method_name, (ranked_idx, scores) in methods.items():\n",
    "    top_idx = ranked_idx[0]\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Job Title: {job_titles[top_idx]}\")\n",
    "    print(f\"  Score: {scores[top_idx]:.3f}\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATION ===\")\n",
    "print(\"\\nFor job title semantic similarity, here's the ranking of methods:\")\n",
    "print(\"\\n1. **GloVe + Cosine Similarity** (BEST CHOICE)\")\n",
    "print(\"   - Excellent semantic understanding\")\n",
    "print(\"   - Good balance of performance and accuracy\")\n",
    "print(\"   - Handles out-of-vocabulary words reasonably\")\n",
    "\n",
    "print(\"2. **Word2Vec + Cosine Similarity** (Second Choice)\")\n",
    "print(\"   - Strong semantic relationships\")\n",
    "print(\"   - Trained on Google News, good for professional terms\")\n",
    "\n",
    "print(\"3. **FastText + Cosine Similarity** (Third Choice)\")\n",
    "print(\"   - Handles subword information well\")\n",
    "print(\"   - Good for rare or misspelled words\")\n",
    "\n",
    "print(\"4. **METEOR Score** (Best for text generation evaluation)\")\n",
    "print(\"   - Considers synonyms and stemming\")\n",
    "print(\"   - Better than BLEU for semantic similarity\")\n",
    "\n",
    "print(\"5. **CIDEr Score** (Good for consensus-based evaluation)\")\n",
    "print(\"   - Uses TF-IDF weighting\")\n",
    "print(\"   - Good when you have multiple reference texts\")\n",
    "\n",
    "print(\"6. **TF-IDF + Cosine Similarity** (Baseline)\")\n",
    "print(\"   - Simple and fast\")\n",
    "print(\"   - Limited semantic understanding\")\n",
    "\n",
    "print(\"7. **BLEU Score** (Not recommended for this task)\")\n",
    "print(\"   - Designed for machine translation\")\n",
    "print(\"   - Poor for semantic similarity of short texts\")\n",
    "\n",
    "print(\"\\n**FINAL RECOMMENDATION: Use GloVe + Cosine Similarity**\")\n",
    "print(\"This method provides the best balance of semantic understanding,\")\n",
    "print(\"computational efficiency, and practical performance for job title matching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d08bf",
   "metadata": {},
   "source": [
    "# Simple LLM-based Candidate Ranking using Groq API (Llama 3 70B Versatile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1ba59fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://api.groq.com/openai/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM-ranked job titles:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, llm_output)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43msimple_llm_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_titles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_term\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36msimple_llm_rank\u001b[39m\u001b[34m(job_titles, search_term)\u001b[39m\n\u001b[32m     15\u001b[39m data = {\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mllama3-70b-8192\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}],\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.2\u001b[39m,\n\u001b[32m     19\u001b[39m }\n\u001b[32m     20\u001b[39m response = requests.post(\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://api.groq.com/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m, headers=headers, json=data\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m result = response.json()\n\u001b[32m     25\u001b[39m llm_output = result[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 3\\Potential-Talents\\env\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://api.groq.com/openai/v1/chat/completions"
     ]
    }
   ],
   "source": [
    "# --- Simple LLM-based Candidate Ranking using Groq API (Llama 3 70B Versatile) ---\n",
    "\n",
    "\n",
    "def simple_llm_rank(job_titles, search_term):\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\n",
    "            \"GROQ_API_KEY not found in environment variables. Please set it in your .env file.\"\n",
    "        )\n",
    "    prompt = (\n",
    "        f\"Rank these job titles by how well they match the search term '{search_term}'. Return a numbered list, most relevant first.\\n\"\n",
    "        + \"\\n\".join(job_titles)\n",
    "    )\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"llama3-70b-8192\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "    response = requests.post(\n",
    "        \"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=data\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    llm_output = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"LLM-ranked job titles:\\n\", llm_output)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "simple_llm_rank(job_titles, search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8316614",
   "metadata": {},
   "source": [
    "## 12. Compare Multiple Transformer Models (Gemma, Qwen, etc.)\n",
    "Experiment with different Hugging Face transformer models for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72816f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ranking with model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Student (Score: 0.548)\n",
      "student at eastern university (Score: 0.470)\n",
      "graduate student (Score: 0.461)\n",
      "student at university of southern california (Score: 0.455)\n",
      "student at northeastern university (Score: 0.454)\n",
      "student at northwest missouri state university (Score: 0.453)\n",
      "student at santa clara university (Score: 0.450)\n",
      "student at mississippi state university (Score: 0.445)\n",
      "student at arizona state university (Score: 0.440)\n",
      "student at edgewood college (Score: 0.437)\n",
      "\n",
      "Ranking with model: Qwen/Qwen1.5-0.5B-Chat\n",
      "Student (Score: 0.548)\n",
      "student at eastern university (Score: 0.470)\n",
      "graduate student (Score: 0.461)\n",
      "student at university of southern california (Score: 0.455)\n",
      "student at northeastern university (Score: 0.454)\n",
      "student at northwest missouri state university (Score: 0.453)\n",
      "student at santa clara university (Score: 0.450)\n",
      "student at mississippi state university (Score: 0.445)\n",
      "student at arizona state university (Score: 0.440)\n",
      "student at edgewood college (Score: 0.437)\n",
      "\n",
      "Ranking with model: Qwen/Qwen1.5-0.5B-Chat\n",
      "systems analyst (Score: 0.956)\n",
      "writer researcher developer (Score: 0.937)\n",
      "graduate (Score: 0.922)\n",
      "senior analyst (Score: 0.921)\n",
      "financial analyst (Score: 0.916)\n",
      "senior data engineer (Score: 0.904)\n",
      "system administrator security analyst network admin desktop support grc auditor (Score: 0.903)\n",
      "senior data spatial scientist (Score: 0.894)\n",
      "financial services professional data scientist (Score: 0.882)\n",
      "senior data engineeranalytics (Score: 0.878)\n",
      "\n",
      "Ranking with model: bert-base-uncased\n",
      "systems analyst (Score: 0.956)\n",
      "writer researcher developer (Score: 0.937)\n",
      "graduate (Score: 0.922)\n",
      "senior analyst (Score: 0.921)\n",
      "financial analyst (Score: 0.916)\n",
      "senior data engineer (Score: 0.904)\n",
      "system administrator security analyst network admin desktop support grc auditor (Score: 0.903)\n",
      "senior data spatial scientist (Score: 0.894)\n",
      "financial services professional data scientist (Score: 0.882)\n",
      "senior data engineeranalytics (Score: 0.878)\n",
      "\n",
      "Ranking with model: bert-base-uncased\n",
      "Student (Score: 0.579)\n",
      "researcher (Score: 0.521)\n",
      "researcher (Score: 0.521)\n",
      "researcher (Score: 0.521)\n",
      "graduate student (Score: 0.501)\n",
      "graduate (Score: 0.490)\n",
      "student at saint peters university (Score: 0.486)\n",
      "masters student at franklin university (Score: 0.484)\n",
      "m.s. data science (Score: 0.483)\n",
      "graduate student umd (Score: 0.483)\n",
      "Student (Score: 0.579)\n",
      "researcher (Score: 0.521)\n",
      "researcher (Score: 0.521)\n",
      "researcher (Score: 0.521)\n",
      "graduate student (Score: 0.501)\n",
      "graduate (Score: 0.490)\n",
      "student at saint peters university (Score: 0.486)\n",
      "masters student at franklin university (Score: 0.484)\n",
      "m.s. data science (Score: 0.483)\n",
      "graduate student umd (Score: 0.483)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_transformer_embeddings(model_name, texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        output = model(**encoded)\n",
    "        embeddings = output.last_hidden_state.mean(dim=1).numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",  # SBERT baseline\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",  # Qwen (smaller, efficient)\n",
    "    \"bert-base-uncased\",  # Classic BERT model\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nRanking with model: {model_name}\")\n",
    "    job_embs = get_transformer_embeddings(model_name, job_titles)\n",
    "    search_emb = get_transformer_embeddings(model_name, [search_term])\n",
    "    sims = cosine_similarity(search_emb, job_embs).flatten()\n",
    "    top_idx = np.argsort(sims)[::-1]\n",
    "    for idx in top_idx[:10]:\n",
    "        print(f\"{job_titles[idx]} (Score: {sims[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a88bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess and select the best performing transformer model\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for model_name in model_names:\n",
    "    job_embs = get_transformer_embeddings(model_name, job_titles)\n",
    "    search_emb = get_transformer_embeddings(model_name, [search_term])\n",
    "    sims = cosine_similarity(search_emb, job_embs).flatten()\n",
    "    top_idx = np.argsort(sims)[::-1][:10]\n",
    "    avg_top_score = sims[top_idx].mean()\n",
    "    results.append(\n",
    "        {\n",
    "            \"model\": model_name,\n",
    "            \"avg_top10_similarity\": avg_top_score,\n",
    "            \"top_job_titles\": [job_titles[i] for i in top_idx],\n",
    "            \"top_scores\": [sims[i] for i in top_idx],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Transformer Model Comparison ===\")\n",
    "print(results_df[[\"model\", \"avg_top10_similarity\"]])\n",
    "\n",
    "best_model = results_df.loc[results_df[\"avg_top10_similarity\"].idxmax()]\n",
    "print(f\"\\nBest performing model: {best_model['model']}\")\n",
    "print(\"Top 10 job titles:\")\n",
    "for title, score in zip(best_model[\"top_job_titles\"], best_model[\"top_scores\"]):\n",
    "    print(f\"{title} (Score: {score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146e8a7",
   "metadata": {},
   "source": [
    "## 13. Fine-tune Best Transformer Model with LoRA (Parameter-Efficient Fine-Tuning)\n",
    "Now we will fine-tune the best performing transformer model using the LoRA (Low-Rank Adaptation) technique for parameter-efficient fine-tuning, leveraging the extended Potential Talents dataset. This approach allows us to adapt large models with minimal additional parameters and compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4160134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training pairs...\n",
      "Processing title 0/1281\n",
      "Processing title 100/1281\n",
      "Processing title 100/1281\n",
      "Processing title 200/1281\n",
      "Processing title 200/1281\n",
      "Processing title 300/1281\n",
      "Processing title 300/1281\n",
      "Processing title 400/1281\n",
      "Processing title 400/1281\n",
      "Processing title 500/1281\n",
      "Processing title 500/1281\n",
      "Processing title 600/1281\n",
      "Processing title 600/1281\n",
      "Processing title 700/1281\n",
      "Processing title 700/1281\n",
      "Processing title 800/1281\n",
      "Processing title 800/1281\n",
      "Processing title 900/1281\n",
      "Processing title 900/1281\n",
      "Processing title 1000/1281\n",
      "Processing title 1000/1281\n",
      "Processing title 1100/1281\n",
      "Processing title 1100/1281\n",
      "Processing title 1200/1281\n",
      "Processing title 1200/1281\n",
      "Model structure:\n",
      "\n",
      "embed_tokens\n",
      "layers\n",
      "layers.0\n",
      "layers.0.self_attn\n",
      "layers.0.mlp\n",
      "layers.0.input_layernorm\n",
      "layers.0.post_attention_layernorm\n",
      "layers.1\n",
      "layers.1.self_attn\n",
      "layers.1.mlp\n",
      "layers.1.input_layernorm\n",
      "layers.1.post_attention_layernorm\n",
      "layers.2\n",
      "layers.2.self_attn\n",
      "layers.2.mlp\n",
      "layers.2.input_layernorm\n",
      "layers.2.post_attention_layernorm\n",
      "layers.3\n",
      "layers.3.self_attn\n",
      "layers.3.mlp\n",
      "layers.3.input_layernorm\n",
      "layers.3.post_attention_layernorm\n",
      "layers.4\n",
      "layers.4.self_attn\n",
      "layers.4.mlp\n",
      "layers.4.input_layernorm\n",
      "layers.4.post_attention_layernorm\n",
      "layers.5\n",
      "layers.5.self_attn\n",
      "\n",
      "Attention modules:\n",
      "layers.0.self_attn\n",
      "layers.1.self_attn\n",
      "layers.2.self_attn\n",
      "layers.3.self_attn\n",
      "layers.4.self_attn\n",
      "layers.5.self_attn\n",
      "layers.6.self_attn\n",
      "layers.7.self_attn\n",
      "layers.8.self_attn\n",
      "layers.9.self_attn\n",
      "Trainable parameters: 1179648\n",
      "Model structure:\n",
      "\n",
      "embed_tokens\n",
      "layers\n",
      "layers.0\n",
      "layers.0.self_attn\n",
      "layers.0.mlp\n",
      "layers.0.input_layernorm\n",
      "layers.0.post_attention_layernorm\n",
      "layers.1\n",
      "layers.1.self_attn\n",
      "layers.1.mlp\n",
      "layers.1.input_layernorm\n",
      "layers.1.post_attention_layernorm\n",
      "layers.2\n",
      "layers.2.self_attn\n",
      "layers.2.mlp\n",
      "layers.2.input_layernorm\n",
      "layers.2.post_attention_layernorm\n",
      "layers.3\n",
      "layers.3.self_attn\n",
      "layers.3.mlp\n",
      "layers.3.input_layernorm\n",
      "layers.3.post_attention_layernorm\n",
      "layers.4\n",
      "layers.4.self_attn\n",
      "layers.4.mlp\n",
      "layers.4.input_layernorm\n",
      "layers.4.post_attention_layernorm\n",
      "layers.5\n",
      "layers.5.self_attn\n",
      "\n",
      "Attention modules:\n",
      "layers.0.self_attn\n",
      "layers.1.self_attn\n",
      "layers.2.self_attn\n",
      "layers.3.self_attn\n",
      "layers.4.self_attn\n",
      "layers.5.self_attn\n",
      "layers.6.self_attn\n",
      "layers.7.self_attn\n",
      "layers.8.self_attn\n",
      "layers.9.self_attn\n",
      "Trainable parameters: 1179648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 593\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m    592\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m optimizer.step()\n\u001b[32m    596\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 3\\Potential-Talents\\env\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 3\\Potential-Talents\\env\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 3\\Potential-Talents\\env\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from peft import get_peft_model, LoraConfig, TaskType\n",
    "# from datasets import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.optim import AdamW\n",
    "# from scipy.stats import spearmanr\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "# import nltk\n",
    "\n",
    "# # Download NLTK data\n",
    "# nltk.download(\"punkt\", quiet=True)\n",
    "# nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "# # Load the extended dataset\n",
    "# df = pd.read_excel(\"Extended Dataset for Potential Talents 1.xlsx\")\n",
    "\n",
    "# # Identify job title column\n",
    "# possible_columns = [\n",
    "#     \"job_title\",\n",
    "#     \"title\",\n",
    "#     \"position\",\n",
    "#     \"role\",\n",
    "#     \"job\",\n",
    "#     \"designation\",\n",
    "#     \"job title\",\n",
    "# ]\n",
    "# job_title_column = None\n",
    "# for col in df.columns:\n",
    "#     if any(keyword in col.lower() for keyword in possible_columns):\n",
    "#         job_title_column = col\n",
    "#         break\n",
    "# if not job_title_column:\n",
    "#     raise ValueError(\"Job title column not found. Please specify it manually.\")\n",
    "\n",
    "# # Extract job titles\n",
    "# job_titles = df[job_title_column].dropna().astype(str).tolist()\n",
    "\n",
    "\n",
    "# # Create pairs for training (similar and dissimilar pairs)\n",
    "# def create_training_pairs(titles, num_pairs=5000):\n",
    "#     pairs = []\n",
    "#     labels = []\n",
    "\n",
    "#     # Calculate METEOR scores between all pairs to determine similarity\n",
    "#     for i, title1 in enumerate(titles):\n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"Processing title {i}/{len(titles)}\")\n",
    "\n",
    "#         # Only process a subset of titles for efficiency\n",
    "#         sample_indices = np.random.choice(\n",
    "#             [j for j in range(len(titles)) if j != i],\n",
    "#             min(20, len(titles) - 1),\n",
    "#             replace=False,\n",
    "#         )\n",
    "\n",
    "#         for j in sample_indices:\n",
    "#             title2 = titles[j]\n",
    "#             # Calculate METEOR score\n",
    "#             score = meteor_score(\n",
    "#                 [nltk.word_tokenize(title1.lower())], nltk.word_tokenize(title2.lower())\n",
    "#             )\n",
    "\n",
    "#             # Add pair with score as label (regression task)\n",
    "#             pairs.append((title1, title2))\n",
    "#             labels.append(score)\n",
    "\n",
    "#     # Convert to numpy arrays and shuffle\n",
    "#     pairs = np.array(pairs)\n",
    "#     labels = np.array(labels)\n",
    "#     indices = np.random.permutation(len(pairs))\n",
    "\n",
    "#     return pairs[indices][:num_pairs], labels[indices][:num_pairs]\n",
    "\n",
    "\n",
    "# # Create training pairs\n",
    "# print(\"Creating training pairs...\")\n",
    "# pairs, similarity_scores = create_training_pairs(job_titles)\n",
    "\n",
    "# # Split into train and validation sets\n",
    "# train_pairs, val_pairs, train_scores, val_scores = train_test_split(\n",
    "#     pairs, similarity_scores, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "# # Prepare datasets\n",
    "# def prepare_dataset(pairs, scores):\n",
    "#     return Dataset.from_dict(\n",
    "#         {\"text_a\": pairs[:, 0], \"text_b\": pairs[:, 1], \"score\": scores}\n",
    "#     )\n",
    "\n",
    "\n",
    "# train_dataset = prepare_dataset(train_pairs, train_scores)\n",
    "# val_dataset = prepare_dataset(val_pairs, val_scores)\n",
    "\n",
    "# # Use a model that works better with LoRA\n",
    "# best_model_name = \"bert-base-uncased\"  # Changed from Qwen to BERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(best_model_name)\n",
    "# base_model = AutoModel.from_pretrained(best_model_name)\n",
    "\n",
    "\n",
    "# # Function to inspect model structure\n",
    "# def print_model_structure(model):\n",
    "#     print(\"Model structure:\")\n",
    "#     for name, _ in list(model.named_modules())[:30]:  # Print first 30 layers\n",
    "#         print(name)\n",
    "\n",
    "\n",
    "# # Print model structure to identify correct layer names\n",
    "# print_model_structure(base_model)\n",
    "\n",
    "# # Define LoRA configuration with correct target modules\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.FEATURE_EXTRACTION,\n",
    "#     inference_mode=False,\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.1,\n",
    "#     # For BERT models, use these target modules:\n",
    "#     target_modules=[\"query\", \"key\", \"value\"],\n",
    "# )\n",
    "\n",
    "# # Create LoRA model\n",
    "# try:\n",
    "#     model = get_peft_model(base_model, peft_config)\n",
    "#     print(\n",
    "#         f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"Error with target modules: {e}\")\n",
    "#     # Try alternative target modules based on printed structure\n",
    "#     print(\"Trying alternative target modules...\")\n",
    "#     peft_config = LoraConfig(\n",
    "#         task_type=TaskType.FEATURE_EXTRACTION,\n",
    "#         inference_mode=False,\n",
    "#         r=8,\n",
    "#         lora_alpha=32,\n",
    "#         lora_dropout=0.1,\n",
    "#         # Alternative target modules for BERT\n",
    "#         target_modules=[\n",
    "#             \"attention.self.query\",\n",
    "#             \"attention.self.key\",\n",
    "#             \"attention.self.value\",\n",
    "#         ],\n",
    "#     )\n",
    "#     model = get_peft_model(base_model, peft_config)\n",
    "#     print(\n",
    "#         f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Define a custom model with similarity prediction head\n",
    "# class SimilarityModel(torch.nn.Module):\n",
    "#     def __init__(self, encoder_model):\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder_model\n",
    "#         self.similarity_head = torch.nn.Linear(encoder_model.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, **inputs):\n",
    "#         # Process first text\n",
    "#         input_ids_a = inputs.pop(\"input_ids_a\")\n",
    "#         attention_mask_a = inputs.pop(\"attention_mask_a\")\n",
    "#         outputs_a = self.encoder(input_ids=input_ids_a, attention_mask=attention_mask_a)\n",
    "#         embeddings_a = outputs_a.last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Process second text\n",
    "#         input_ids_b = inputs.pop(\"input_ids_b\")\n",
    "#         attention_mask_b = inputs.pop(\"attention_mask_b\")\n",
    "#         outputs_b = self.encoder(input_ids=input_ids_b, attention_mask=attention_mask_b)\n",
    "#         embeddings_b = outputs_b.last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Calculate similarity score\n",
    "#         similarity = self.similarity_head(embeddings_a * embeddings_b)\n",
    "#         return similarity.squeeze()\n",
    "\n",
    "\n",
    "# # Create the similarity model\n",
    "# similarity_model = SimilarityModel(model)\n",
    "\n",
    "\n",
    "# # Prepare data collator\n",
    "# def collate_fn(batch):\n",
    "#     text_a = [item[\"text_a\"] for item in batch]\n",
    "#     text_b = [item[\"text_b\"] for item in batch]\n",
    "#     scores = [item[\"score\"] for item in batch]\n",
    "\n",
    "#     # Tokenize\n",
    "#     encoded_a = tokenizer(text_a, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     encoded_b = tokenizer(text_b, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids_a\": encoded_a[\"input_ids\"],\n",
    "#         \"attention_mask_a\": encoded_a[\"attention_mask\"],\n",
    "#         \"input_ids_b\": encoded_b[\"input_ids\"],\n",
    "#         \"attention_mask_b\": encoded_b[\"attention_mask\"],\n",
    "#         \"labels\": torch.tensor(scores, dtype=torch.float),\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "# # Training setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# similarity_model.to(device)\n",
    "# optimizer = AdamW(similarity_model.parameters(), lr=5e-5)\n",
    "# loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 3\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     similarity_model.train()\n",
    "#     train_loss = 0\n",
    "#     for batch in train_loader:\n",
    "#         # Move batch to device\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         labels = batch.pop(\"labels\")\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = similarity_model(**batch)\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     # Validation\n",
    "#     similarity_model.eval()\n",
    "#     val_loss = 0\n",
    "#     predictions = []\n",
    "#     true_scores = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             # Move batch to device\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             labels = batch.pop(\"labels\")\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = similarity_model(**batch)\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             predictions.extend(outputs.cpu().numpy())\n",
    "#             true_scores.extend(labels.cpu().numpy())\n",
    "\n",
    "#     # Calculate correlation\n",
    "#     correlation, _ = spearmanr(true_scores, predictions)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "#     print(f\"  Train Loss: {train_loss / len(train_loader):.4f}\")\n",
    "#     print(f\"  Val Loss: {val_loss / len(val_loader):.4f}\")\n",
    "#     print(f\"  Rank Correlation: {correlation:.4f}\")\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# model.save_pretrained(\"finetuned_job_title_model\")\n",
    "\n",
    "# # Evaluate on test set\n",
    "# print(\"\\nEvaluating fine-tuned model on job title matching task...\")\n",
    "\n",
    "\n",
    "# # Function to get embeddings from fine-tuned model\n",
    "# def get_finetuned_embeddings(texts):\n",
    "#     similarity_model.eval()\n",
    "#     tokenizer_output = tokenizer(\n",
    "#         texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "#     )\n",
    "#     tokenizer_output = {k: v.to(device) for k, v in tokenizer_output.items()}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**tokenizer_output)\n",
    "#         embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "# # Select a search term\n",
    "# search_term = job_titles[np.random.randint(0, len(job_titles))]\n",
    "# print(f\"Search term: {search_term}\")\n",
    "\n",
    "# # Get embeddings and calculate similarity\n",
    "# job_embs = get_finetuned_embeddings(job_titles)\n",
    "# search_emb = get_finetuned_embeddings([search_term])\n",
    "# sims = cosine_similarity(search_emb, job_embs).flatten()\n",
    "# top_idx = np.argsort(sims)[::-1]\n",
    "\n",
    "# # Calculate METEOR scores for comparison\n",
    "# meteor_scores = [\n",
    "#     meteor_score(\n",
    "#         [nltk.word_tokenize(search_term.lower())], nltk.word_tokenize(title.lower())\n",
    "#     )\n",
    "#     for title in job_titles\n",
    "# ]\n",
    "# meteor_top_idx = np.argsort(meteor_scores)[::-1]\n",
    "\n",
    "# # Calculate precision@20\n",
    "# finetuned_top20 = set(top_idx[:20])\n",
    "# meteor_top20 = set(meteor_top_idx[:20])\n",
    "# precision_at_20 = len(finetuned_top20.intersection(meteor_top20)) / 20\n",
    "\n",
    "# # Calculate rank correlation\n",
    "# rank_correlation, _ = spearmanr(\n",
    "#     [list(meteor_top_idx).index(i) for i in range(len(job_titles))],\n",
    "#     [list(top_idx).index(i) for i in range(len(job_titles))],\n",
    "# )\n",
    "\n",
    "# print(\"\\n=== FINE-TUNED MODEL PERFORMANCE ===\")\n",
    "# print(f\"Precision@20: {precision_at_20:.3f}\")\n",
    "# print(f\"Rank Correlation: {rank_correlation:.3f}\")\n",
    "\n",
    "# # Print top 10 results\n",
    "# print(\"\\nTop 10 job titles by fine-tuned model:\")\n",
    "# for i, idx in enumerate(top_idx[:10]):\n",
    "#     print(f\"{i + 1}. {job_titles[idx]} (Score: {sims[idx]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1e15b",
   "metadata": {},
   "source": [
    "# Fine Tuning Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24749b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 2060\n",
      "Loaded et_data.xlsx for training\n",
      "Loaded 1281 job titles for training\n",
      "Loaded potential-talents.xlsx for testing\n",
      "Loaded 104 job titles for testing\n",
      "Creating training pairs from 1281 titles...\n",
      "Limited to 500 titles for faster processing\n",
      "Processing 0/500 - Time elapsed: 0.0s\n",
      "Processing 50/500 - Time elapsed: 0.1s\n",
      "Processing 100/500 - Time elapsed: 0.2s\n",
      "Loaded et_data.xlsx for training\n",
      "Loaded 1281 job titles for training\n",
      "Loaded potential-talents.xlsx for testing\n",
      "Loaded 104 job titles for testing\n",
      "Creating training pairs from 1281 titles...\n",
      "Limited to 500 titles for faster processing\n",
      "Processing 0/500 - Time elapsed: 0.0s\n",
      "Processing 50/500 - Time elapsed: 0.1s\n",
      "Processing 100/500 - Time elapsed: 0.2s\n",
      "Processing 150/500 - Time elapsed: 0.3s\n",
      "Processing 200/500 - Time elapsed: 0.4s\n",
      "Processing 250/500 - Time elapsed: 0.5s\n",
      "Processing 300/500 - Time elapsed: 0.6s\n",
      "Processing 350/500 - Time elapsed: 0.8s\n",
      "Processing 400/500 - Time elapsed: 0.9s\n",
      "Processing 450/500 - Time elapsed: 1.0s\n",
      "Created 500 training pairs in 1.1s\n",
      "Using device: cuda\n",
      "Loading model...\n",
      "Processing 150/500 - Time elapsed: 0.3s\n",
      "Processing 200/500 - Time elapsed: 0.4s\n",
      "Processing 250/500 - Time elapsed: 0.5s\n",
      "Processing 300/500 - Time elapsed: 0.6s\n",
      "Processing 350/500 - Time elapsed: 0.8s\n",
      "Processing 400/500 - Time elapsed: 0.9s\n",
      "Processing 450/500 - Time elapsed: 1.0s\n",
      "Created 500 training pairs in 1.1s\n",
      "Using device: cuda\n",
      "Loading model...\n",
      "sentence-transformers/all-MiniLM-L6-v2 loaded successfully\n",
      "Finding valid target modules for LoRA...\n",
      "Found 24 valid target modules: ['encoder.layer.0.attention.self.query', 'encoder.layer.0.attention.self.key', 'encoder.layer.0.attention.self.value']...\n",
      "LoRA applied successfully\n",
      "Similarity model created successfully\n",
      "\n",
      "Starting epoch 1/2\n",
      "  Batch 0/100 - Loss: 0.0207\n",
      "sentence-transformers/all-MiniLM-L6-v2 loaded successfully\n",
      "Finding valid target modules for LoRA...\n",
      "Found 24 valid target modules: ['encoder.layer.0.attention.self.query', 'encoder.layer.0.attention.self.key', 'encoder.layer.0.attention.self.value']...\n",
      "LoRA applied successfully\n",
      "Similarity model created successfully\n",
      "\n",
      "Starting epoch 1/2\n",
      "  Batch 0/100 - Loss: 0.0207\n",
      "  Batch 5/100 - Loss: 0.0275\n",
      "  Batch 10/100 - Loss: 0.0234\n",
      "  Batch 5/100 - Loss: 0.0275\n",
      "  Batch 10/100 - Loss: 0.0234\n",
      "  Batch 15/100 - Loss: 0.1376\n",
      "  Batch 20/100 - Loss: 0.0273\n",
      "  Batch 15/100 - Loss: 0.1376\n",
      "  Batch 20/100 - Loss: 0.0273\n",
      "  Batch 25/100 - Loss: 0.0383\n",
      "  Batch 30/100 - Loss: 0.0301\n",
      "  Batch 25/100 - Loss: 0.0383\n",
      "  Batch 30/100 - Loss: 0.0301\n",
      "  Batch 35/100 - Loss: 0.0317\n",
      "  Batch 40/100 - Loss: 0.0050\n",
      "  Batch 35/100 - Loss: 0.0317\n",
      "  Batch 40/100 - Loss: 0.0050\n",
      "  Batch 45/100 - Loss: 0.0034\n",
      "  Batch 50/100 - Loss: 0.0059\n",
      "  Batch 45/100 - Loss: 0.0034\n",
      "  Batch 50/100 - Loss: 0.0059\n",
      "  Batch 55/100 - Loss: 0.0068\n",
      "  Batch 60/100 - Loss: 0.0135\n",
      "  Batch 55/100 - Loss: 0.0068\n",
      "  Batch 60/100 - Loss: 0.0135\n",
      "  Batch 65/100 - Loss: 0.0140\n",
      "  Batch 70/100 - Loss: 0.0052\n",
      "  Batch 65/100 - Loss: 0.0140\n",
      "  Batch 70/100 - Loss: 0.0052\n",
      "  Batch 75/100 - Loss: 0.0032\n",
      "  Batch 80/100 - Loss: 0.0012\n",
      "  Batch 75/100 - Loss: 0.0032\n",
      "  Batch 80/100 - Loss: 0.0012\n",
      "  Batch 85/100 - Loss: 0.0520\n",
      "  Batch 90/100 - Loss: 0.0021\n",
      "  Batch 85/100 - Loss: 0.0520\n",
      "  Batch 90/100 - Loss: 0.0021\n",
      "  Batch 95/100 - Loss: 0.0289\n",
      "Epoch 1 Train Loss: 0.0159\n",
      "  Batch 95/100 - Loss: 0.0289\n",
      "Epoch 1 Train Loss: 0.0159\n",
      "Epoch 1 Val Loss: 0.0124 | Spearman: 0.419\n",
      "\n",
      "Starting epoch 2/2\n",
      "  Batch 0/100 - Loss: 0.0040\n",
      "  Batch 5/100 - Loss: 0.0054\n",
      "Epoch 1 Val Loss: 0.0124 | Spearman: 0.419\n",
      "\n",
      "Starting epoch 2/2\n",
      "  Batch 0/100 - Loss: 0.0040\n",
      "  Batch 5/100 - Loss: 0.0054\n",
      "  Batch 10/100 - Loss: 0.0077\n",
      "  Batch 15/100 - Loss: 0.0191\n",
      "  Batch 10/100 - Loss: 0.0077\n",
      "  Batch 15/100 - Loss: 0.0191\n",
      "  Batch 20/100 - Loss: 0.0021\n",
      "  Batch 25/100 - Loss: 0.0026\n",
      "  Batch 20/100 - Loss: 0.0021\n",
      "  Batch 25/100 - Loss: 0.0026\n",
      "  Batch 30/100 - Loss: 0.0105\n",
      "  Batch 35/100 - Loss: 0.0035\n",
      "  Batch 30/100 - Loss: 0.0105\n",
      "  Batch 35/100 - Loss: 0.0035\n",
      "  Batch 40/100 - Loss: 0.0080\n",
      "  Batch 45/100 - Loss: 0.0039\n",
      "  Batch 40/100 - Loss: 0.0080\n",
      "  Batch 45/100 - Loss: 0.0039\n",
      "  Batch 50/100 - Loss: 0.0029\n",
      "  Batch 55/100 - Loss: 0.0048\n",
      "  Batch 50/100 - Loss: 0.0029\n",
      "  Batch 55/100 - Loss: 0.0048\n",
      "  Batch 60/100 - Loss: 0.0081\n",
      "  Batch 65/100 - Loss: 0.0043\n",
      "  Batch 60/100 - Loss: 0.0081\n",
      "  Batch 65/100 - Loss: 0.0043\n",
      "  Batch 70/100 - Loss: 0.0150\n",
      "  Batch 75/100 - Loss: 0.0018\n",
      "  Batch 70/100 - Loss: 0.0150\n",
      "  Batch 75/100 - Loss: 0.0018\n",
      "  Batch 80/100 - Loss: 0.0022\n",
      "  Batch 85/100 - Loss: 0.0037\n",
      "  Batch 80/100 - Loss: 0.0022\n",
      "  Batch 85/100 - Loss: 0.0037\n",
      "  Batch 90/100 - Loss: 0.0037\n",
      "  Batch 95/100 - Loss: 0.0036\n",
      "  Batch 90/100 - Loss: 0.0037\n",
      "  Batch 95/100 - Loss: 0.0036\n",
      "Epoch 2 Train Loss: 0.0079\n",
      "Epoch 2 Train Loss: 0.0079\n",
      "Epoch 2 Val Loss: 0.0107 | Spearman: 0.447\n",
      "Epoch 2 Val Loss: 0.0107 | Spearman: 0.447\n",
      "Model saved successfully to finetuned_job_title_model/\n",
      "\n",
      "Test search term: Student\n",
      "Getting embeddings for test job titles...\n",
      "Getting embedding for search term...\n",
      "Calculating similarities...\n",
      "\n",
      "Top 10 job titles by fine-tuned model:\n",
      "1. Student (Score: 0.691)\n",
      "2. Student at Westfield State University (Score: 0.526)\n",
      "3. Student at Chapman University (Score: 0.492)\n",
      "4. Student at Chapman University (Score: 0.492)\n",
      "5. Student at Chapman University (Score: 0.492)\n",
      "6. Student at Chapman University (Score: 0.492)\n",
      "7. Student at Indiana University Kokomo - Business Management - \n",
      "Retail Manager at Delphi Hardware and Paint (Score: 0.451)\n",
      "8. Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "9. Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "10. Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "\n",
      "Script completed successfully!\n",
      "Model saved successfully to finetuned_job_title_model/\n",
      "\n",
      "Test search term: Student\n",
      "Getting embeddings for test job titles...\n",
      "Getting embedding for search term...\n",
      "Calculating similarities...\n",
      "\n",
      "Top 10 job titles by fine-tuned model:\n",
      "1. Student (Score: 0.691)\n",
      "2. Student at Westfield State University (Score: 0.526)\n",
      "3. Student at Chapman University (Score: 0.492)\n",
      "4. Student at Chapman University (Score: 0.492)\n",
      "5. Student at Chapman University (Score: 0.492)\n",
      "6. Student at Chapman University (Score: 0.492)\n",
      "7. Student at Indiana University Kokomo - Business Management - \n",
      "Retail Manager at Delphi Hardware and Paint (Score: 0.451)\n",
      "8. Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "9. Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "10. Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "\n",
      "Script completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Print debug info about environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Download required NLTK resources for tokenization and METEOR score\n",
    "nltk.download(\"punkt\", quiet=True)  # For tokenization\n",
    "nltk.download(\"wordnet\", quiet=True)  # For METEOR score (synonym matching)\n",
    "\n",
    "# --- 1. Load Training Data (et_data.xlsx) ---\n",
    "# This section loads the training data from et_data.xlsx\n",
    "try:\n",
    "    train_df = pd.read_excel(\"et_data.xlsx\")\n",
    "    print(\"Loaded et_data.xlsx for training\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: et_data.xlsx not found. This file is required for training.\")\n",
    "    raise\n",
    "\n",
    "# Automatically detect the job title column by looking for common column name patterns\n",
    "job_title_column = None\n",
    "for col in train_df.columns:\n",
    "    if any(k in col.lower() for k in [\"job_title\", \"title\", \"position\", \"role\"]):\n",
    "        job_title_column = col\n",
    "        break\n",
    "if not job_title_column:\n",
    "    # If no column found, use the first column as fallback\n",
    "    job_title_column = train_df.columns[0]\n",
    "    print(\n",
    "        f\"No job title column found in training data, using first column: {job_title_column}\"\n",
    "    )\n",
    "\n",
    "# Extract job titles as a list, removing any missing values\n",
    "train_job_titles = train_df[job_title_column].dropna().astype(str).tolist()\n",
    "print(f\"Loaded {len(train_job_titles)} job titles for training\")\n",
    "\n",
    "# --- Load Test Data (potential-talents.xlsx) ---\n",
    "# This section loads the test data from potential-talents.xlsx\n",
    "try:\n",
    "    test_df = pd.read_excel(\"potential-talents.xlsx\")\n",
    "    print(\"Loaded potential-talents.xlsx for testing\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        \"Warning: potential-talents.xlsx not found. Will use training data for testing.\"\n",
    "    )\n",
    "    test_df = train_df\n",
    "\n",
    "# Automatically detect the job title column in test data\n",
    "test_job_title_column = None\n",
    "for col in test_df.columns:\n",
    "    if any(k in col.lower() for k in [\"job_title\", \"title\", \"position\", \"role\"]):\n",
    "        test_job_title_column = col\n",
    "        break\n",
    "if not test_job_title_column:\n",
    "    # If no column found, use the first column as fallback\n",
    "    test_job_title_column = test_df.columns[0]\n",
    "    print(\n",
    "        f\"No job title column found in test data, using first column: {test_job_title_column}\"\n",
    "    )\n",
    "\n",
    "# Extract test job titles as a list\n",
    "test_job_titles = test_df[test_job_title_column].dropna().astype(str).tolist()\n",
    "print(f\"Loaded {len(test_job_titles)} job titles for testing\")\n",
    "\n",
    "\n",
    "# --- 2. Create Training Pairs (with safeguards) ---\n",
    "def create_training_pairs(titles, num_pairs=500):  # Reduced from 2000 to 500 for speed\n",
    "    \"\"\"\n",
    "    Creates pairs of job titles with similarity scores for training.\n",
    "    Uses METEOR score as the similarity metric between pairs.\n",
    "\n",
    "    Args:\n",
    "        titles: List of job title strings\n",
    "        num_pairs: Maximum number of pairs to create\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (pairs, scores) where:\n",
    "            - pairs is a numpy array of (title1, title2) tuples\n",
    "            - scores is a numpy array of similarity scores\n",
    "    \"\"\"\n",
    "    print(f\"Creating training pairs from {len(titles)} titles...\")\n",
    "    start_time = time.time()\n",
    "    pairs, labels = [], []\n",
    "\n",
    "    # Limit number of titles to process for speed\n",
    "    max_titles = min(500, len(titles))\n",
    "    titles = titles[:max_titles]\n",
    "    print(f\"Limited to {max_titles} titles for faster processing\")\n",
    "\n",
    "    for i, t1 in enumerate(titles):\n",
    "        if i % 50 == 0:  # Print progress updates\n",
    "            print(\n",
    "                f\"Processing {i}/{len(titles)} - Time elapsed: {time.time() - start_time:.1f}s\"\n",
    "            )\n",
    "\n",
    "        # For each title, compare with a small random sample of other titles\n",
    "        idxs = np.random.choice(\n",
    "            [j for j in range(len(titles)) if j != i],\n",
    "            min(5, len(titles) - 1),  # Only compare with 5 other titles\n",
    "            replace=False,\n",
    "        )\n",
    "\n",
    "        for j in idxs:\n",
    "            t2 = titles[j]\n",
    "            try:\n",
    "                # Calculate METEOR score between the two titles\n",
    "                # METEOR considers synonyms, stemming, and word order\n",
    "                score = meteor_score(\n",
    "                    [nltk.word_tokenize(t1.lower())], nltk.word_tokenize(t2.lower())\n",
    "                )\n",
    "                pairs.append((t1, t2))\n",
    "                labels.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with pair ({t1}, {t2}): {e}\")\n",
    "                # Use a default score instead of skipping to maintain data volume\n",
    "                pairs.append((t1, t2))\n",
    "                labels.append(0.5)  # Default mid-range score\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        raise ValueError(\"No pairs created. Check your data and NLTK setup.\")\n",
    "\n",
    "    # Randomly shuffle and limit to requested number of pairs\n",
    "    arr = np.random.permutation(len(pairs))\n",
    "    final_pairs = np.array(pairs)[arr][:num_pairs]\n",
    "    final_labels = np.array(labels)[arr][:num_pairs]\n",
    "    print(\n",
    "        f\"Created {len(final_pairs)} training pairs in {time.time() - start_time:.1f}s\"\n",
    "    )\n",
    "    return final_pairs, final_labels\n",
    "\n",
    "\n",
    "# Create training pairs from et_data.xlsx with error handling\n",
    "try:\n",
    "    # Generate pairs and their similarity scores\n",
    "    pairs, scores = create_training_pairs(train_job_titles)\n",
    "    # Split into training (80%) and validation (20%) sets\n",
    "    split = int(0.8 * len(pairs))\n",
    "    train_pairs, val_pairs = pairs[:split], pairs[split:]\n",
    "    train_scores, val_scores = scores[:split], scores[split:]\n",
    "except Exception as e:\n",
    "    print(f\"Error creating pairs: {e}\")\n",
    "    # Create dummy data as fallback to allow training to continue\n",
    "    print(\"Creating dummy training data as fallback\")\n",
    "    dummy_pairs = [\n",
    "        (train_job_titles[i], train_job_titles[j])\n",
    "        for i in range(min(10, len(train_job_titles)))\n",
    "        for j in range(min(10, len(train_job_titles)))\n",
    "        if i != j\n",
    "    ]\n",
    "    dummy_scores = [0.5] * len(dummy_pairs)\n",
    "    train_pairs = dummy_pairs[:80]\n",
    "    val_pairs = dummy_pairs[80:100]\n",
    "    train_scores = dummy_scores[:80]\n",
    "    val_scores = dummy_scores[80:100]\n",
    "\n",
    "\n",
    "# Custom Dataset class for job title pairs\n",
    "class PairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for pairs of job titles with similarity scores.\n",
    "    Each item is a dictionary with text_a, text_b, and score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pairs, scores):\n",
    "        self.pairs = pairs\n",
    "        self.scores = scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text_a\": self.pairs[idx][0],\n",
    "            \"text_b\": self.pairs[idx][1],\n",
    "            \"score\": self.scores[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# Create PyTorch datasets for training and validation\n",
    "train_dataset = PairDataset(train_pairs, train_scores)\n",
    "val_dataset = PairDataset(val_pairs, val_scores)\n",
    "\n",
    "# --- 3. Model & LoRA ---\n",
    "# Set up device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load a reliable model with error handling\n",
    "try:\n",
    "    print(\"Loading model...\")\n",
    "    # Use sentence-transformers model which is designed for semantic similarity\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    base_model = AutoModel.from_pretrained(model_name)\n",
    "    print(f\"{model_name} loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # Fall back to a smaller, more widely available model\n",
    "    print(\"Falling back to smaller model: distilbert-base-uncased\")\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Find valid target modules for LoRA fine-tuning\n",
    "# LoRA works by adding low-rank adapters to specific layers (usually attention)\n",
    "print(\"Finding valid target modules for LoRA...\")\n",
    "valid_modules = []\n",
    "for name, module in base_model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        # Focus on attention modules first as they're most important for adaptation\n",
    "        if any(\n",
    "            key in name.lower()\n",
    "            for key in [\n",
    "                \"attention\",\n",
    "                \"attn\",\n",
    "                \"query\",\n",
    "                \"key\",\n",
    "                \"value\",\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "            ]\n",
    "        ):\n",
    "            valid_modules.append(name)\n",
    "\n",
    "if not valid_modules:\n",
    "    # If no attention modules found, use any Linear layer as fallback\n",
    "    for name, module in base_model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            valid_modules.append(name)\n",
    "            if len(valid_modules) >= 5:  # Limit to 5 modules\n",
    "                break\n",
    "\n",
    "print(f\"Found {len(valid_modules)} valid target modules: {valid_modules[:3]}...\")\n",
    "\n",
    "# Configure LoRA with the found modules\n",
    "# LoRA is a parameter-efficient fine-tuning technique that adds small\n",
    "# trainable matrices to existing weights instead of updating all parameters\n",
    "try:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,  # For embedding models\n",
    "        inference_mode=False,  # We're training, not inferring\n",
    "        r=8,  # Rank of LoRA adaptation matrices (smaller = fewer parameters)\n",
    "        lora_alpha=32,  # Scaling factor for LoRA\n",
    "        lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "        target_modules=valid_modules,  # Which modules to apply LoRA to\n",
    "    )\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    print(\"LoRA applied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying LoRA: {e}\")\n",
    "    print(\"Using base model without LoRA\")\n",
    "    model = base_model\n",
    "\n",
    "\n",
    "# Define a model that computes similarity between two texts\n",
    "class SimilarityModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model that computes similarity between two texts using a shared encoder.\n",
    "    Takes two texts, encodes both, and computes similarity with a linear head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        # Get the correct hidden size from the model config\n",
    "        if hasattr(encoder, \"config\"):\n",
    "            hidden_size = encoder.config.hidden_size\n",
    "        else:\n",
    "            # Fallback for models without standard config\n",
    "            hidden_size = 768  # Common default size\n",
    "        # Linear layer that takes element-wise product of embeddings and outputs a score\n",
    "        self.head = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "        # Handle different model output formats (models can return outputs differently)\n",
    "        try:\n",
    "            # Try standard format (last_hidden_state attribute)\n",
    "            out_a = self.encoder(\n",
    "                input_ids=input_ids_a, attention_mask=attention_mask_a\n",
    "            ).last_hidden_state.mean(dim=1)\n",
    "\n",
    "            out_b = self.encoder(\n",
    "                input_ids=input_ids_b, attention_mask=attention_mask_b\n",
    "            ).last_hidden_state.mean(dim=1)\n",
    "        except AttributeError:\n",
    "            # Alternative output format (tuple where first element is hidden states)\n",
    "            out_a = self.encoder(\n",
    "                input_ids=input_ids_a, attention_mask=attention_mask_a\n",
    "            )[0].mean(dim=1)\n",
    "\n",
    "            out_b = self.encoder(\n",
    "                input_ids=input_ids_b, attention_mask=attention_mask_b\n",
    "            )[0].mean(dim=1)\n",
    "\n",
    "        # Element-wise product combines the two embeddings\n",
    "        # This captures how similar the embeddings are in each dimension\n",
    "        sim = self.head(out_a * out_b)\n",
    "        return sim.squeeze()\n",
    "\n",
    "\n",
    "# Create the similarity model with error handling\n",
    "try:\n",
    "    similarity_model = SimilarityModel(model).to(device)\n",
    "    print(\"Similarity model created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating similarity model: {e}\")\n",
    "\n",
    "    # Create a simplified model as fallback\n",
    "    class SimpleSimilarityModel(torch.nn.Module):\n",
    "        \"\"\"Simplified fallback model with basic embedding layer\"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.embedding = torch.nn.Embedding(tokenizer.vocab_size, 128)\n",
    "            self.head = torch.nn.Linear(128, 1)\n",
    "\n",
    "        def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "            emb_a = self.embedding(input_ids_a).mean(dim=1)\n",
    "            emb_b = self.embedding(input_ids_b).mean(dim=1)\n",
    "            return self.head(emb_a * emb_b).squeeze()\n",
    "\n",
    "    similarity_model = SimpleSimilarityModel().to(device)\n",
    "    print(\"Using simplified fallback model\")\n",
    "\n",
    "\n",
    "# Collate function for DataLoader to batch samples together\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader that tokenizes text pairs and prepares tensors.\n",
    "    Handles errors by creating dummy tensors if tokenization fails.\n",
    "    \"\"\"\n",
    "    text_a = [b[\"text_a\"] for b in batch]\n",
    "    text_b = [b[\"text_b\"] for b in batch]\n",
    "    scores = [b[\"score\"] for b in batch]\n",
    "\n",
    "    try:\n",
    "        # Tokenize both texts in the pair\n",
    "        enc_a = tokenizer(text_a, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        enc_b = tokenizer(text_b, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenization: {e}\")\n",
    "        # Create dummy tensors as fallback\n",
    "        enc_a = {\n",
    "            \"input_ids\": torch.ones(len(text_a), 10).long(),\n",
    "            \"attention_mask\": torch.ones(len(text_a), 10),\n",
    "        }\n",
    "        enc_b = {\n",
    "            \"input_ids\": torch.ones(len(text_b), 10).long(),\n",
    "            \"attention_mask\": torch.ones(len(text_b), 10),\n",
    "        }\n",
    "\n",
    "    # Return a dictionary with all inputs needed for the model\n",
    "    return {\n",
    "        \"input_ids_a\": enc_a[\"input_ids\"],\n",
    "        \"attention_mask_a\": enc_a[\"attention_mask\"],\n",
    "        \"input_ids_b\": enc_b[\"input_ids\"],\n",
    "        \"attention_mask_b\": enc_b[\"attention_mask\"],\n",
    "        \"labels\": torch.tensor(scores, dtype=torch.float),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "# Use smaller batch size and fewer workers for stability\n",
    "batch_size = 4  # Small batch size to avoid OOM errors\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # Use main process only to avoid multiprocessing issues\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # Use main process only\n",
    ")\n",
    "\n",
    "# --- 4. Training with error handling ---\n",
    "# Set up optimizer and loss function\n",
    "optimizer = AdamW(\n",
    "    similarity_model.parameters(), lr=5e-5\n",
    ")  # AdamW optimizer with small learning rate\n",
    "loss_fn = torch.nn.MSELoss()  # Mean Squared Error for regression task\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):  # 2 epochs for quick training (increase for better results)\n",
    "    print(f\"\\nStarting epoch {epoch + 1}/2\")\n",
    "    similarity_model.train()  # Set model to training mode\n",
    "    train_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    # Process each batch with error handling\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        try:\n",
    "            # Move batch to device (GPU/CPU)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop(\"labels\")  # Extract labels\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            out = similarity_model(**batch)  # Get model predictions\n",
    "            loss = loss_fn(out, labels)  # Calculate loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            # Print progress\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(\n",
    "                    f\"  Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in training batch {batch_idx}: {e}\")\n",
    "            continue  # Skip problematic batch and continue\n",
    "\n",
    "    # Print epoch summary\n",
    "    avg_loss = train_loss / max(1, batch_count)\n",
    "    print(f\"Epoch {epoch + 1} Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    similarity_model.eval()  # Set model to evaluation mode\n",
    "    val_loss, preds, trues = 0, [], []\n",
    "    val_batch_count = 0\n",
    "\n",
    "    # Process validation data without computing gradients\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            try:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                labels = batch.pop(\"labels\")\n",
    "                out = similarity_model(**batch)\n",
    "                loss = loss_fn(out, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_batch_count += 1\n",
    "                preds.extend(out.cpu().numpy())  # Save predictions\n",
    "                trues.extend(labels.cpu().numpy())  # Save ground truth\n",
    "            except Exception as e:\n",
    "                print(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Calculate and print validation metrics\n",
    "    if preds and trues:\n",
    "        try:\n",
    "            # Spearman correlation measures how well the rankings match\n",
    "            corr, _ = spearmanr(trues, preds)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1} Val Loss: {val_loss / max(1, val_batch_count):.4f} | Spearman: {corr:.3f}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating correlation: {e}\")\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1} Val Loss: {val_loss / max(1, val_batch_count):.4f}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} Val Loss: {val_loss / max(1, val_batch_count):.4f} | No valid predictions\"\n",
    "        )\n",
    "\n",
    "# --- 5. Save the model ---\n",
    "try:\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(\"finetuned_job_title_model\", exist_ok=True)\n",
    "\n",
    "    # Save the encoder model (with LoRA weights)\n",
    "    similarity_model.encoder.save_pretrained(\"finetuned_job_title_model\")\n",
    "    # Save the tokenizer for later use\n",
    "    tokenizer.save_pretrained(\"finetuned_job_title_model\")\n",
    "\n",
    "    # Save the similarity head separately (not part of the transformer model)\n",
    "    torch.save(\n",
    "        similarity_model.head.state_dict(),\n",
    "        \"finetuned_job_title_model/similarity_head.pt\",\n",
    "    )\n",
    "\n",
    "    print(\"Model saved successfully to finetuned_job_title_model/\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "\n",
    "# --- 6. Test on Job Title Matching with error handling ---\n",
    "def get_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Get embeddings for a list of texts using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        texts: List of text strings to encode\n",
    "\n",
    "    Returns:\n",
    "        Numpy array of embeddings, shape (len(texts), embedding_dim)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similarity_model.eval()  # Set to evaluation mode\n",
    "        # Tokenize the texts\n",
    "        enc = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(\n",
    "            device\n",
    "        )\n",
    "        # Get embeddings without computing gradients\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Try the standard output format\n",
    "                out = model(**enc).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            except (AttributeError, TypeError):\n",
    "                # Try alternative output format\n",
    "                out = model(**enc)[0].mean(dim=1).cpu().numpy()\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings: {e}\")\n",
    "        # Return random embeddings as fallback\n",
    "        return np.random.randn(len(texts), 128)\n",
    "\n",
    "\n",
    "# Test the model on the test dataset\n",
    "try:\n",
    "    search_term = \"Student\"  # Fixed search term for consistent testing\n",
    "    print(\"\\nTest search term:\", search_term)\n",
    "\n",
    "    print(\"Getting embeddings for test job titles...\")\n",
    "    job_embs = get_embeddings(test_job_titles)\n",
    "\n",
    "    print(\"Getting embedding for search term...\")\n",
    "    search_emb = get_embeddings([search_term])\n",
    "\n",
    "    print(\"Calculating similarities...\")\n",
    "    # Cosine similarity between search term and all job titles\n",
    "    sims = cosine_similarity(search_emb, job_embs).flatten()\n",
    "\n",
    "    # Sort by similarity (descending) and print top results\n",
    "    top_idx = np.argsort(sims)[::-1]\n",
    "    print(\"\\nTop 10 job titles by fine-tuned model:\")\n",
    "    for i, idx in enumerate(top_idx[:10]):\n",
    "        print(f\"{i + 1}. {test_job_titles[idx]} (Score: {sims[idx]:.3f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in final evaluation: {e}\")\n",
    "    # Show random job titles as fallback\n",
    "    print(\"Showing random job titles instead:\")\n",
    "    indices = np.random.choice(len(test_job_titles), 10, replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        print(f\"{i + 1}. {test_job_titles[idx]}\")\n",
    "\n",
    "print(\"\\nScript completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
