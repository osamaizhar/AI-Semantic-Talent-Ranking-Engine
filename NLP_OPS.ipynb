{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8268e07",
   "metadata": {},
   "source": [
    "# NLP Operations: Job Title Matching\n",
    "This notebook demonstrates various NLP techniques to vectorize job titles and a search term, and then ranks candidates by similarity. Techniques covered:\n",
    "- TF-IDF\n",
    "- Word2Vec (Google)\n",
    "- GloVe\n",
    "- FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52638f5",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Install and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840e55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Osama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from dotenv import load_dotenv\n",
    "from utils import bleu_score\n",
    "from collections import Counter\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    ")  # Import METEOR function from utils\n",
    "from utils import meteor\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Import custom utility functions\n",
    "\n",
    "# For CIDEr, we'll implement a simplified version\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735f83d",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Load job titles from the Excel file and define a search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0b36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected search term: Student\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"potential-talents.xlsx\")\n",
    "possible_columns = [\n",
    "    \"job_title\",\n",
    "    \"title\",\n",
    "    \"position\",\n",
    "    \"role\",\n",
    "    \"job\",\n",
    "    \"designation\",\n",
    "    \"job title\",\n",
    "]\n",
    "job_title_column = None\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_columns):\n",
    "        job_title_column = col\n",
    "        break\n",
    "if not job_title_column:\n",
    "    raise ValueError(\"Job title column not found. Please specify it manually.\")\n",
    "job_titles = df[job_title_column].dropna().astype(str).tolist()\n",
    "\n",
    "# Filter job titles to only those with 1 or 2 words\n",
    "filtered_job_titles = [title for title in job_titles if 1 <= len(title.split()) <= 2]\n",
    "\n",
    "# Randomly select a search term from filtered job titles\n",
    "if filtered_job_titles:\n",
    "    search_term = random.choice(filtered_job_titles)\n",
    "else:\n",
    "    raise ValueError(\"No job titles with 1 or 2 words found.\")\n",
    "\n",
    "print(f\"Randomly selected search term: {search_term}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2b628",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Vectorization & Cosine Similarity\n",
    "Vectorize job titles and search term using TF-IDF, then rank candidates by similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78bd229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by TF-IDF similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Chapman University (Score: 0.455)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.371)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = job_titles + [search_term]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "search_vec = X[-1]\n",
    "job_vecs = X[:-1]\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by TF-IDF similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af1a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- LLM-based Candidate Ranking using Groq API (Llama 3 70B Versatile) ---\n",
    "# import requests\n",
    "# import os\n",
    "# import json\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# def rank_candidates_with_llm(\n",
    "#     job_titles, search_term, model=\"llama3-70b-8192\"\n",
    "# ):  # Llama 3 70B Versatile\n",
    "#     \"\"\"\n",
    "#     Use Groq LLM API to rank job titles by relevance to the search term.\n",
    "#     Args:\n",
    "#         job_titles (list): List of job title strings.\n",
    "#         search_term (str): The search term/job title to match against.\n",
    "#         model (str): Groq model name (default: llama3-70b-8192).\n",
    "#     Returns:\n",
    "#         list: Ranked job titles (most relevant first).\n",
    "#     \"\"\"\n",
    "#     api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "#     if not api_key:\n",
    "#         raise ValueError(\n",
    "#             \"GROQ_API_KEY not found in environment variables. Please set it in your .env file.\"\n",
    "#         )\n",
    "#     prompt = f\"\"\"\n",
    "# You are an expert recruiter. Given the following list of candidate job titles, rank them from most to least relevant for the search term: '{search_term}'.\\n\\nJob Titles:\\n\"\"\"\n",
    "#     for i, title in enumerate(job_titles, 1):\n",
    "#         prompt += f\"{i}. {title}\\n\"\n",
    "#     prompt += \"\\nReturn the ranking as a numbered list, most relevant first. Only include the job titles.\"\n",
    "\n",
    "#     headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "#     data = {\n",
    "#         \"model\": model,\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#         \"temperature\": 0.2,\n",
    "#     }\n",
    "#     response = requests.post(\n",
    "#         \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "#         headers=headers,\n",
    "#         data=json.dumps(data),\n",
    "#     )\n",
    "#     response.raise_for_status()\n",
    "#     result = response.json()\n",
    "#     llm_output = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "#     # Parse the LLM output into a ranked list\n",
    "#     ranked = [\n",
    "#         line.split(\". \", 1)[-1].strip()\n",
    "#         for line in llm_output.split(\"\\n\")\n",
    "#         if line.strip() and line[0].isdigit()\n",
    "#     ]\n",
    "#     return ranked\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # ranked_list = rank_candidates_with_llm(job_titles, search_term)\n",
    "# # print(\"LLM-ranked job titles:\")\n",
    "# # for title in ranked_list:\n",
    "# #     print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0288f",
   "metadata": {},
   "source": [
    "## 4. Word2Vec (Google News) Vectorization & Cosine Similarity\n",
    "Vectorize using pre-trained Google News Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6bc6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected search term: Student\n",
      "Top 10 job titles by Word2Vec similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Chapman University (Score: 0.807)\n",
      "Student at Westfield State University (Score: 0.793)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.575)\n"
     ]
    }
   ],
   "source": [
    "# Download Google News vectors (only needs to be done once)\n",
    "# w2v = api.load('word2vec-google-news-300')\n",
    "w2v = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "\n",
    "def get_w2v_vector(text, model):\n",
    "    words = [w for w in nltk.word_tokenize(text.lower()) if w in model]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[w] for w in words], axis=0)\n",
    "\n",
    "\n",
    "# Load the Excel file containing potential talents data\n",
    "# (Assumes the file is in the same directory as the notebook)\n",
    "df = pd.read_excel(\"potential-talents.xlsx\")\n",
    "\n",
    "# List of possible column names that may contain job titles\n",
    "possible_columns = [\n",
    "    \"job_title\",\n",
    "    \"title\",\n",
    "    \"position\",\n",
    "    \"role\",\n",
    "    \"job\",\n",
    "    \"designation\",\n",
    "    \"job title\",\n",
    "]\n",
    "\n",
    "# Initialize variable to store the detected job title column name\n",
    "job_title_column = None\n",
    "# Loop through columns in the DataFrame to find a matching job title column\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_columns):\n",
    "        job_title_column = col  # Set the column name if a match is found\n",
    "        break\n",
    "# Raise an error if no job title column is found\n",
    "default_job_title_error = \"Job title column not found. Please specify it manually.\"\n",
    "if not job_title_column:\n",
    "    raise ValueError(default_job_title_error)\n",
    "\n",
    "# Extract job titles as a list of strings, dropping missing values\n",
    "job_titles = df[job_title_column].dropna().astype(str).tolist()\n",
    "\n",
    "# Filter job titles to only those with 1 or 2 words\n",
    "filtered_job_titles = [title for title in job_titles if 1 <= len(title.split()) <= 2]\n",
    "\n",
    "# Randomly select a search term from filtered job titles\n",
    "if filtered_job_titles:\n",
    "    search_term = random.choice(filtered_job_titles)\n",
    "else:\n",
    "    raise ValueError(\"No job titles with 1 or 2 words found.\")\n",
    "\n",
    "# Print the randomly selected search term\n",
    "print(f\"Randomly selected search term: {search_term}\")\n",
    "\n",
    "job_vecs = np.array([get_w2v_vector(title, w2v) for title in job_titles])\n",
    "search_vec = get_w2v_vector(search_term, w2v).reshape(1, -1)\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by Word2Vec similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e7eef",
   "metadata": {},
   "source": [
    "## 5. GloVe Vectorization & Cosine Similarity\n",
    "Vectorize using pre-trained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d2bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by GloVe similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Chapman University (Score: 0.766)\n",
      "Student at Westfield State University (Score: 0.699)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.669)\n"
     ]
    }
   ],
   "source": [
    "# Download GloVe vectors (only needs to be done once)\n",
    "# glove = api.load('glove-wiki-gigaword-300')\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "\n",
    "def get_glove_vector(text, model):\n",
    "    words = [w for w in nltk.word_tokenize(text.lower()) if w in model]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[w] for w in words], axis=0)\n",
    "\n",
    "\n",
    "job_vecs = np.array([get_glove_vector(title, glove) for title in job_titles])\n",
    "search_vec = get_glove_vector(search_term, glove).reshape(1, -1)\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by GloVe similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c981e80",
   "metadata": {},
   "source": [
    "## 6. FastText Vectorization & Cosine Similarity\n",
    "Vectorize using pre-trained FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5538c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by FastText similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (Score: 0.724)\n",
      "Student at Chapman University (Score: 0.709)\n",
      "Student at Chapman University (Score: 0.709)\n"
     ]
    }
   ],
   "source": [
    "# Download FastText vectors (only needs to be done once)\n",
    "# fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "\n",
    "def get_fasttext_vector(text, model):\n",
    "    words = [w for w in nltk.word_tokenize(text.lower()) if w in model]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[w] for w in words], axis=0)\n",
    "\n",
    "\n",
    "job_vecs = np.array(\n",
    "    [get_fasttext_vector(title, fasttext_model) for title in job_titles]\n",
    ")\n",
    "search_vec = get_fasttext_vector(search_term, fasttext_model).reshape(1, -1)\n",
    "similarities = cosine_similarity(search_vec, job_vecs).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "print(\"Top 10 job titles by FastText similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82dca8",
   "metadata": {},
   "source": [
    "## 11. Transformer-based Contextual Embeddings (BERT/Sentence-BERT)\n",
    "Use Sentence-BERT to generate contextual embeddings for job titles and the search term, then rank by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd69527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by SBERT similarity to search term:\n",
      "Student (Score: 1.000)\n",
      "Student at Westfield State University (Score: 0.616)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Chapman University (Score: 0.602)\n",
      "Student at Indiana University Kokomo - Business Management - \n",
      "Retail Manager at Delphi Hardware and Paint (Score: 0.409)\n",
      "Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "Advisory Board Member at Celal Bayar University (Score: 0.398)\n",
      "Advisory Board Member at Celal Bayar University (Score: 0.398)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained Sentence-BERT model\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute embeddings\n",
    "job_embeddings = sbert_model.encode(job_titles)\n",
    "search_embedding = sbert_model.encode([search_term])\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(search_embedding, job_embeddings).flatten()\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "print(\"Top 10 job titles by SBERT similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (Score: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51b2e2",
   "metadata": {},
   "source": [
    "## 7. BLEU Score Calculation\n",
    "Calculate BLEU score for semantic similarity between search term and job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by BLEU semantic similarity to search term:\n",
      "Student (BLEU Score: 1.000)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Chapman University (BLEU Score: 0.061)\n",
      "Student at Westfield State University (BLEU Score: 0.046)\n",
      "Aspiring Human Resources Management student seeking an internship (BLEU Score: 0.029)\n",
      "Aspiring Human Resources Management student seeking an internship (BLEU Score: 0.029)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (BLEU Score: 0.026)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (BLEU Score: 0.026)\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score for each job title against the search term\n",
    "smoothie = SmoothingFunction().method4\n",
    "search_tokens = nltk.word_tokenize(search_term.lower())\n",
    "bleu_scores = [\n",
    "    sentence_bleu(\n",
    "        [search_tokens], nltk.word_tokenize(title.lower()), smoothing_function=smoothie\n",
    "    )\n",
    "    for title in job_titles\n",
    "]\n",
    "ranked_indices = np.argsort(bleu_scores)[::-1]\n",
    "print(\"Top 10 job titles by BLEU semantic similarity to search term:\")\n",
    "for idx in ranked_indices[:10]:\n",
    "    print(f\"{job_titles[idx]} (BLEU Score: {bleu_scores[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f4e3d",
   "metadata": {},
   "source": [
    "## 8. METEOR Score Calculation\n",
    "Calculate METEOR score for semantic similarity. METEOR considers synonyms and stemming, making it more suitable for semantic similarity than BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c5f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by METEOR semantic similarity to search term:\n",
      "Student (METEOR Score: 0.500)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Chapman University (METEOR Score: 0.385)\n",
      "Student at Westfield State University (METEOR Score: 0.357)\n",
      "Aspiring Human Resources Management student seeking an internship (METEOR Score: 0.294)\n",
      "Aspiring Human Resources Management student seeking an internship (METEOR Score: 0.294)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (METEOR Score: 0.278)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (METEOR Score: 0.278)\n"
     ]
    }
   ],
   "source": [
    "# Calculate METEOR score for each job title against the search term\n",
    "search_tokens = nltk.word_tokenize(search_term.lower())\n",
    "meteor_scores = [\n",
    "    meteor_score([search_tokens], nltk.word_tokenize(title.lower()))\n",
    "    for title in job_titles\n",
    "]\n",
    "meteor_rank = np.argsort(meteor_scores)[::-1]\n",
    "\n",
    "print(\"Top 10 job titles by METEOR semantic similarity to search term:\")\n",
    "for idx in meteor_rank[:10]:\n",
    "    print(f\"{job_titles[idx]} (METEOR Score: {meteor_scores[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f5a8b",
   "metadata": {},
   "source": [
    "## 9. CIDEr Score Calculation\n",
    "Calculate CIDEr (Consensus-based Image Description Evaluation) score. Originally for image captioning, but useful for semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 job titles by CIDEr semantic similarity to search term:\n",
      "Student (CIDEr Score: 1.000)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Humber College and Aspiring Human Resources Generalist (CIDEr Score: 0.125)\n",
      "Student at Chapman University (CIDEr Score: 0.105)\n",
      "Student at Chapman University (CIDEr Score: 0.105)\n"
     ]
    }
   ],
   "source": [
    "# Import optimized CiderScorer from utils\n",
    "from utils import CiderScorer\n",
    "\n",
    "# Instantiate one CiderScorer with job_titles once to avoid O(N²) IDF recomputation\n",
    "cider = CiderScorer(job_titles)\n",
    "cider_scores = [cider.score(search_term, t) for t in job_titles]\n",
    "cider_rank = np.argsort(cider_scores)[::-1]\n",
    "\n",
    "print(\"Top 10 job titles by CIDEr semantic similarity to search term:\")\n",
    "for idx in cider_rank[:10]:\n",
    "    print(f\"{job_titles[idx]} (CIDEr Score: {cider_scores[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e9c7a",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Metric Comparison\n",
    "Compare all methods and recommend the best approach for job title semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f2b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE COMPARISON OF SEMANTIC SIMILARITY METRICS ===\n",
      "\n",
      "Top match from each method:\n",
      "\n",
      "TF-IDF + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "Word2Vec + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "GloVe + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "FastText + Cosine:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "BLEU Score:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "METEOR Score:\n",
      "  Job Title: Student\n",
      "  Score: 0.500\n",
      "\n",
      "CIDEr Score:\n",
      "  Job Title: Student\n",
      "  Score: 1.000\n",
      "\n",
      "=== RECOMMENDATION ===\n",
      "\n",
      "For job title semantic similarity, here's the ranking of methods:\n",
      "\n",
      "1. **GloVe + Cosine Similarity** (BEST CHOICE)\n",
      "   - Excellent semantic understanding\n",
      "   - Good balance of performance and accuracy\n",
      "   - Handles out-of-vocabulary words reasonably\n",
      "2. **Word2Vec + Cosine Similarity** (Second Choice)\n",
      "   - Strong semantic relationships\n",
      "   - Trained on Google News, good for professional terms\n",
      "3. **FastText + Cosine Similarity** (Third Choice)\n",
      "   - Handles subword information well\n",
      "   - Good for rare or misspelled words\n",
      "4. **METEOR Score** (Best for text generation evaluation)\n",
      "   - Considers synonyms and stemming\n",
      "   - Better than BLEU for semantic similarity\n",
      "5. **CIDEr Score** (Good for consensus-based evaluation)\n",
      "   - Uses TF-IDF weighting\n",
      "   - Good when you have multiple reference texts\n",
      "6. **TF-IDF + Cosine Similarity** (Baseline)\n",
      "   - Simple and fast\n",
      "   - Limited semantic understanding\n",
      "7. **BLEU Score** (Not recommended for this task)\n",
      "   - Designed for machine translation\n",
      "   - Poor for semantic similarity of short texts\n",
      "\n",
      "**FINAL RECOMMENDATION: Use GloVe + Cosine Similarity**\n",
      "This method provides the best balance of semantic understanding,\n",
      "computational efficiency, and practical performance for job title matching.\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive comparison\n",
    "print(\"=== COMPREHENSIVE COMPARISON OF SEMANTIC SIMILARITY METRICS ===\\n\")\n",
    "\n",
    "# Get top result from each method\n",
    "methods = {\n",
    "    \"TF-IDF + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"Word2Vec + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"GloVe + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"FastText + Cosine\": (np.argsort(similarities)[::-1], similarities),\n",
    "    \"BLEU Score\": (np.argsort(bleu_scores)[::-1], bleu_scores),\n",
    "    \"METEOR Score\": (np.argsort(meteor_scores)[::-1], meteor_scores),\n",
    "    \"CIDEr Score\": (np.argsort(cider_scores)[::-1], cider_scores),\n",
    "}\n",
    "\n",
    "print(\"Top match from each method:\")\n",
    "for method_name, (ranked_idx, scores) in methods.items():\n",
    "    top_idx = ranked_idx[0]\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Job Title: {job_titles[top_idx]}\")\n",
    "    print(f\"  Score: {scores[top_idx]:.3f}\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATION ===\")\n",
    "print(\"\\nFor job title semantic similarity, here's the ranking of methods:\")\n",
    "print(\"\\n1. **GloVe + Cosine Similarity** (BEST CHOICE)\")\n",
    "print(\"   - Excellent semantic understanding\")\n",
    "print(\"   - Good balance of performance and accuracy\")\n",
    "print(\"   - Handles out-of-vocabulary words reasonably\")\n",
    "\n",
    "print(\"2. **Word2Vec + Cosine Similarity** (Second Choice)\")\n",
    "print(\"   - Strong semantic relationships\")\n",
    "print(\"   - Trained on Google News, good for professional terms\")\n",
    "\n",
    "print(\"3. **FastText + Cosine Similarity** (Third Choice)\")\n",
    "print(\"   - Handles subword information well\")\n",
    "print(\"   - Good for rare or misspelled words\")\n",
    "\n",
    "print(\"4. **METEOR Score** (Best for text generation evaluation)\")\n",
    "print(\"   - Considers synonyms and stemming\")\n",
    "print(\"   - Better than BLEU for semantic similarity\")\n",
    "\n",
    "print(\"5. **CIDEr Score** (Good for consensus-based evaluation)\")\n",
    "print(\"   - Uses TF-IDF weighting\")\n",
    "print(\"   - Good when you have multiple reference texts\")\n",
    "\n",
    "print(\"6. **TF-IDF + Cosine Similarity** (Baseline)\")\n",
    "print(\"   - Simple and fast\")\n",
    "print(\"   - Limited semantic understanding\")\n",
    "\n",
    "print(\"7. **BLEU Score** (Not recommended for this task)\")\n",
    "print(\"   - Designed for machine translation\")\n",
    "print(\"   - Poor for semantic similarity of short texts\")\n",
    "\n",
    "print(\"\\n**FINAL RECOMMENDATION: Use GloVe + Cosine Similarity**\")\n",
    "print(\"This method provides the best balance of semantic understanding,\")\n",
    "print(\"computational efficiency, and practical performance for job title matching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d08bf",
   "metadata": {},
   "source": [
    "# Simple LLM-based Candidate Ranking using Groq API (Llama 3 70B Versatile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba59fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-ranked job titles:\n",
      " Here is the list of job titles ranked by how well they match the search term 'Student', with the most relevant first:\n",
      "\n",
      "1. Student\n",
      "2. Student at Westfield State University\n",
      "3. Student at Indiana University Kokomo - Business Management - \n",
      "4. Student at Humber College and Aspiring Human Resources Generalist\n",
      "5. Student at Humber College and Aspiring Human Resources Generalist\n",
      "6. Student at Chapman University\n",
      "7. Aspiring Human Resources Management student seeking an internship\n",
      "8. Aspiring Human Resources Management student seeking an internship\n",
      "9. Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "10. Business Management Major and Aspiring Human Resources Manager.\n",
      "\n",
      "The remaining job titles do not contain the word \"Student\" and are therefore less relevant to the search term.\n"
     ]
    }
   ],
   "source": [
    "# --- Simple LLM-based Candidate Ranking using Groq API (Llama 3 70B Versatile) ---\n",
    "\n",
    "\n",
    "def simple_llm_rank(job_titles, search_term):\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\n",
    "            \"GROQ_API_KEY not found in environment variables. Please set it in your .env file.\"\n",
    "        )\n",
    "    prompt = (\n",
    "        f\"Rank these job titles by how well they match the search term '{search_term}'. Return a numbered list, most relevant first.\\n\"\n",
    "        + \"\\n\".join(job_titles)\n",
    "    )\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"llama3-70b-8192\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "    response = requests.post(\n",
    "        \"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=data\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    llm_output = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"LLM-ranked job titles:\\n\", llm_output)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "simple_llm_rank(job_titles, search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8316614",
   "metadata": {},
   "source": [
    "## 12. Compare Multiple Transformer Models (Gemma, Qwen, etc.)\n",
    "Experiment with different Hugging Face transformer models for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72816f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meteor_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m model_performance = {}\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Use METEOR scores as reference ranking (considered most accurate for this task)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m reference_ranking = np.argsort(\u001b[43mmeteor_scores\u001b[49m)[::-\u001b[32m1\u001b[39m]\n\u001b[32m     56\u001b[39m reference_top20 = \u001b[38;5;28mset\u001b[39m(reference_ranking[:\u001b[32m20\u001b[39m])\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n",
      "\u001b[31mNameError\u001b[39m: name 'meteor_scores' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_transformer_embeddings(model_name, texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        output = model(**encoded)\n",
    "        embeddings = output.last_hidden_state.mean(dim=1).numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",  # SBERT baseline\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",  # Qwen (smaller, efficient)\n",
    "    \"bert-base-uncased\",  # Classic BERT model\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nRanking with model: {model_name}\")\n",
    "    job_embs = get_transformer_embeddings(model_name, job_titles)\n",
    "    search_emb = get_transformer_embeddings(model_name, [search_term])\n",
    "    sims = cosine_similarity(search_emb, job_embs).flatten()\n",
    "    top_idx = np.argsort(sims)[::-1]\n",
    "    for idx in top_idx[:10]:\n",
    "        print(f\"{job_titles[idx]} (Score: {sims[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a88bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transformer Model Comparison ===\n",
      "                                    model  avg_top10_similarity\n",
      "0  sentence-transformers/all-MiniLM-L6-v2              0.433492\n",
      "1                  Qwen/Qwen1.5-0.5B-Chat              0.605420\n",
      "2                       bert-base-uncased              0.495998\n",
      "\n",
      "Best performing model: Qwen/Qwen1.5-0.5B-Chat\n",
      "Top 10 job titles:\n",
      "Human Resources Professional (Score: 0.954)\n",
      "Human Resources Management Major (Score: 0.920)\n",
      "Human Resources Coordinator at InterContinental Buckhead Atlanta (Score: 0.619)\n",
      "Human Resources Coordinator at InterContinental Buckhead Atlanta (Score: 0.619)\n",
      "Human Resources Coordinator at InterContinental Buckhead Atlanta (Score: 0.619)\n",
      "Human Resources Coordinator at InterContinental Buckhead Atlanta (Score: 0.619)\n",
      "Seeking Human Resources Opportunities (Score: 0.510)\n",
      "Seeking Human Resources Opportunities (Score: 0.510)\n",
      "Seeking Human Resources Position (Score: 0.457)\n",
      "Human Resources Specialist at Luxottica (Score: 0.228)\n"
     ]
    }
   ],
   "source": [
    "# Assess and select the best performing transformer model\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for model_name in model_names:\n",
    "    job_embs = get_transformer_embeddings(model_name, job_titles)\n",
    "    search_emb = get_transformer_embeddings(model_name, [search_term])\n",
    "    sims = cosine_similarity(search_emb, job_embs).flatten()\n",
    "    top_idx = np.argsort(sims)[::-1][:10]\n",
    "    avg_top_score = sims[top_idx].mean()\n",
    "    results.append(\n",
    "        {\n",
    "            \"model\": model_name,\n",
    "            \"avg_top10_similarity\": avg_top_score,\n",
    "            \"top_job_titles\": [job_titles[i] for i in top_idx],\n",
    "            \"top_scores\": [sims[i] for i in top_idx],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Transformer Model Comparison ===\")\n",
    "print(results_df[[\"model\", \"avg_top10_similarity\"]])\n",
    "\n",
    "best_model = results_df.loc[results_df[\"avg_top10_similarity\"].idxmax()]\n",
    "print(f\"\\nBest performing model: {best_model['model']}\")\n",
    "print(\"Top 10 job titles:\")\n",
    "for title, score in zip(best_model[\"top_job_titles\"], best_model[\"top_scores\"]):\n",
    "    print(f\"{title} (Score: {score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146e8a7",
   "metadata": {},
   "source": [
    "## 13. Fine-tune Best Transformer Model with LoRA (Parameter-Efficient Fine-Tuning)\n",
    "Now we will fine-tune the best performing transformer model using the LoRA (Low-Rank Adaptation) technique for parameter-efficient fine-tuning, leveraging the extended Potential Talents dataset. This approach allows us to adapt large models with minimal additional parameters and compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4160134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the best transformer model using LoRA (Parameter-Efficient Fine-Tuning)\n",
    "# This example uses the PEFT and transformers libraries. Make sure to install: pip install peft transformers datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset, train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Use the best model from previous cell\n",
    "# target_model_name = best_model['model']\n",
    "target_model_name = (\n",
    "    best_model[\"model\"] if isinstance(best_model, dict) else best_model.model\n",
    ")\n",
    "\n",
    "# Load and prepare the extended Potential Talents dataset\n",
    "# Assume the dataset has columns: 'job_title' and 'label' (label: 1=match, 0=not match)\n",
    "df = pd.read_excel(\"potential-talents.xlsx\")\n",
    "if \"label\" not in df.columns:\n",
    "    raise ValueError(\n",
    "        \"The extended dataset must have a 'label' column for supervised fine-tuning.\"\n",
    "    )\n",
    "\n",
    "# Prepare the dataset for transformers\n",
    "hf_dataset = Dataset.from_pandas(df[[\"job_title\", \"label\"]])\n",
    "train_data, test_data = train_test_split(hf_dataset, test_size=0.2, seed=42)\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(\n",
    "        example[\"job_title\"], truncation=True, padding=\"max_length\", max_length=32\n",
    "    )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "train_data = train_data.map(preprocess)\n",
    "test_data = test_data.map(preprocess)\n",
    "\n",
    "# Load model for sequence classification\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    target_model_name, num_labels=2\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1\n",
    ")\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune with LoRA\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set before saving\n",
    "outputs = trainer.predict(test_data)\n",
    "labels = outputs.label_ids\n",
    "preds = outputs.predictions.argmax(axis=-1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(labels, preds))\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(labels, preds))\n",
    "\n",
    "# Save the LoRA-adapted model\n",
    "peft_model.save_pretrained(\"./lora_finetuned\")\n",
    "print(\"LoRA fine-tuning complete. Model saved to ./lora_finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
